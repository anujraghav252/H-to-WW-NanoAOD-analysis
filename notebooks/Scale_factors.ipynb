{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e376ec3-c4c9-4f6b-859c-f2099761236b",
   "metadata": {},
   "source": [
    "## Scaling Factors Calculation for H $\\to$ WW Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe61c6-26b0-4688-b95a-7498227dc9ad",
   "metadata": {},
   "source": [
    "# Algorithm: \n",
    "\n",
    "## Overview\n",
    "This algorithm calculates MC normalization scale factors, K-factor corrections, and signal strength measurements for Higgs to WW analysis across multiple jet categories.\n",
    "\n",
    "---\n",
    "\n",
    "## INPUT\n",
    "\n",
    "1. **Luminosity** ($\\mathcal{L}$) for run period -- Check the luminosity from official resource\n",
    "2. **Sample information** for each MC sample $i$:\n",
    "   - Cross-section: $\\sigma_i$ (in pb)  -- use the [latinos github](https://github.com/latinos/LatinoAnalysis/blob/master/NanoGardener/python/framework/samples/samplesCrossSections2018.py) for this\n",
    "   - Sum of generator weights: $\\sum w_{\\text{gen},i}$\n",
    "   - Sample type: signal/background/top\n",
    "3. **Raw event yields** for each sample in each category (0Z, 1Z, 2Z)\n",
    "4. **Observed data events** per category\n",
    "\n",
    "---\n",
    "\n",
    "## PART 1: Calculate MC Normalization Scale Factors\n",
    "\n",
    "For each MC sample $i$:\n",
    "\n",
    "### Step 1.1: Convert luminosity from fb$^{-1}$ to pb$^{-1}$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{pb}} = \\mathcal{L} \\times 1000\n",
    "$$\n",
    "\n",
    "### Step 1.2: Calculate scale factor\n",
    "\n",
    "$$\n",
    "\\text{SF}_i = \\frac{\\sigma_i \\times \\mathcal{L}_{\\text{pb}}}{\\sum w_{\\text{gen},i}}\n",
    "$$\n",
    "\n",
    "### Step 1.3: Store scale factor\n",
    "\n",
    "Store $\\text{SF}_i$ for sample $i$\n",
    "\n",
    "---\n",
    "\n",
    "## PART 2: Apply Scale Factors to Raw Event Yields\n",
    "\n",
    "For each category $c \\in \\{0Z, 1Z, 2Z\\}$:\n",
    "\n",
    "### Step 2.1: Get raw event count\n",
    "\n",
    "$$\n",
    "N_{\\text{raw},i,c} = \\text{raw count for sample } i \\text{ in category } c\n",
    "$$\n",
    "\n",
    "### Step 2.2: Apply scale factor\n",
    "\n",
    "$$\n",
    "N_{\\text{scaled},i,c} = N_{\\text{raw},i,c} \\times \\text{SF}_i\n",
    "$$\n",
    "\n",
    "### Step 2.3: Store scaled yield\n",
    "\n",
    "Store $N_{\\text{scaled},i,c}$\n",
    "\n",
    "---\n",
    "\n",
    "## PART 3: Calculate K-factor for Top Background Correction\n",
    "\n",
    "For each category $c$:\n",
    "\n",
    "### Step 3.1: Sum all top MC yields\n",
    "\n",
    "$$\n",
    "N_{\\text{top},c} = \\sum_{i \\in \\text{top samples}} N_{\\text{scaled},i,c}\n",
    "$$\n",
    "\n",
    "### Step 3.2: Sum all non-top MC yields\n",
    "\n",
    "$$\n",
    "N_{\\text{other},c} = \\sum_{i \\notin \\text{top samples}} N_{\\text{scaled},i,c}\n",
    "$$\n",
    "\n",
    "### Step 3.3: Get observed data\n",
    "\n",
    "$$\n",
    "N_{\\text{data},c} = \\text{observed events in category } c\n",
    "$$\n",
    "\n",
    "### Step 3.4: Calculate K-factor\n",
    "\n",
    "$$\n",
    "K_c = \\frac{N_{\\text{data},c} - N_{\\text{other},c}}{N_{\\text{top},c}}\n",
    "$$\n",
    "\n",
    "### Step 3.5: Store K-factor\n",
    "\n",
    "Store $K_c$ for category $c$\n",
    "\n",
    "---\n",
    "\n",
    "## PART 4: Apply K-factor to Top Samples\n",
    "\n",
    "For each category $c$:\n",
    "\n",
    "For each top sample $i$:\n",
    "\n",
    "### Step 4.1: Apply K-factor correction\n",
    "\n",
    "$$\n",
    "N_{\\text{final},i,c} = N_{\\text{scaled},i,c} \\times K_c \\quad \\text{if } i \\in \\text{top samples}\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{\\text{final},i,c} = N_{\\text{scaled},i,c} \\quad \\text{if } i \\notin \\text{top samples}\n",
    "$$\n",
    "\n",
    "### Step 4.2: Store final yield\n",
    "\n",
    "Store $N_{\\text{final},i,c}$\n",
    "\n",
    "---\n",
    "\n",
    "## PART 5: Calculate Signal Strength per Category\n",
    "\n",
    "For each category $c$:\n",
    "\n",
    "### Step 5.1: Sum all signal yields\n",
    "\n",
    "$$\n",
    "N_{\\text{signal},c} = \\sum_{i \\in \\text{signal samples}} N_{\\text{final},i,c}\n",
    "$$\n",
    "\n",
    "### Step 5.2: Sum all background yields (including top)\n",
    "\n",
    "$$\n",
    "N_{\\text{bkg},c} = \\sum_{i \\in \\text{background samples}} N_{\\text{final},i,c}\n",
    "$$\n",
    "\n",
    "### Step 5.3: Get observed events\n",
    "\n",
    "$$\n",
    "N_{\\text{obs},c} = N_{\\text{data},c}\n",
    "$$\n",
    "\n",
    "### Step 5.4: Calculate signal strength\n",
    "\n",
    "$$\n",
    "\\mu_c = \\frac{N_{\\text{obs},c} - N_{\\text{bkg},c}}{N_{\\text{signal},c}}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\mu_c = 1$: Standard Model (SM) expectation\n",
    "- $\\mu_c > 1$: Signal excess over SM\n",
    "- $\\mu_c < 1$: Signal deficit compared to SM\n",
    "\n",
    "### Step 5.5: Calculate statistical uncertainty\n",
    "\n",
    "$$\n",
    "\\sigma_{\\mu,c} = \\frac{\\sqrt{N_{\\text{obs},c}}}{N_{\\text{signal},c}}\n",
    "$$\n",
    "\n",
    "*(Note: This is a simplified Poisson uncertainty. Use Combine tool for proper treatment.)*\n",
    "\n",
    "### Step 5.6: Store results\n",
    "\n",
    "Store $\\mu_c$ and $\\sigma_{\\mu,c}$\n",
    "\n",
    "---\n",
    "\n",
    "## PART 6: Calculate Combined Signal Strength\n",
    "\n",
    "### Step 6.1: Calculate weights for each category\n",
    "\n",
    "Using inverse-variance weighting:\n",
    "\n",
    "$$\n",
    "w_c = \\frac{1}{\\sigma_{\\mu,c}^2}\n",
    "$$\n",
    "\n",
    "### Step 6.2: Calculate weighted average\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{combined}} = \\frac{\\sum_c \\mu_c \\times w_c}{\\sum_c w_c}\n",
    "$$\n",
    "\n",
    "### Step 6.3: Calculate combined uncertainty\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{combined}} = \\frac{1}{\\sqrt{\\sum_c w_c}}\n",
    "$$\n",
    "\n",
    "### Step 6.4: Store combined results\n",
    "\n",
    "Store $\\mu_{\\text{combined}}$ and $\\sigma_{\\text{combined}}$\n",
    "\n",
    "---\n",
    "\n",
    "## PART 7: Output Results\n",
    "\n",
    "### Step 7.1-7.6: Save results to CSV files\n",
    "\n",
    "- `scale_factors.csv`: $\\text{SF}_i$ for all MC samples\n",
    "- `scaled_event_yields.csv`: $N_{\\text{scaled},i,c}$ per category\n",
    "- `k_factors.csv`: $K_c$ per category\n",
    "- `final_event_yields.csv`: $N_{\\text{final},i,c}$ per category\n",
    "- `signal_strength_by_category.csv`: $\\mu_c \\pm \\sigma_{\\mu,c}$\n",
    "- `combined_signal_strength.csv`: $\\mu_{\\text{combined}} \\pm \\sigma_{\\text{combined}}$\n",
    "\n",
    "### Step 7.7: Generate visualization plots\n",
    "\n",
    "Create plots showing:\n",
    "- Signal strength by category with combined result\n",
    "- Event yields comparison (data vs. MC)\n",
    "- K-factor values\n",
    "- Sample composition\n",
    "\n",
    "---\n",
    "\n",
    "## OUTPUT SUMMARY\n",
    "\n",
    "| Output | Description | Formula |\n",
    "|--------|-------------|---------|\n",
    "| **Scale Factors** | MC normalization to luminosity | $\\text{SF}_i = \\frac{\\sigma_i \\times \\mathcal{L} \\times 1000}{\\sum w_{\\text{gen},i}}$ |\n",
    "| **Scaled Yields** | Normalized event counts | $N_{\\text{scaled},i,c} = N_{\\text{raw},i,c} \\times \\text{SF}_i$ |\n",
    "| **K-factors** | Top background data-driven correction | $K_c = \\frac{N_{\\text{data},c} - N_{\\text{other},c}}{N_{\\text{top},c}}$ |\n",
    "| **Final Yields** | K-factor corrected yields | $N_{\\text{final},i,c} = N_{\\text{scaled},i,c} \\times K_c$ (for top) |\n",
    "| **Signal Strength (per category)** | Observed vs. expected signal | $\\mu_c = \\frac{N_{\\text{obs},c} - N_{\\text{bkg},c}}{N_{\\text{signal},c}}$ |\n",
    "| **Combined Signal Strength** | Weighted average across categories | $\\mu_{\\text{combined}} = \\frac{\\sum_c \\mu_c w_c}{\\sum_c w_c}$ where $w_c = \\frac{1}{\\sigma_{\\mu,c}^2}$ |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Equations Summary\n",
    "\n",
    "### 1. MC Normalization\n",
    "$$\n",
    "\\boxed{\\text{SF}_i = \\frac{\\sigma_i \\times \\mathcal{L} \\times 1000}{\\sum w_{\\text{gen},i}}}\n",
    "$$\n",
    "\n",
    "### 2. K-factor (Top Correction)\n",
    "$$\n",
    "\\boxed{K_c = \\frac{N_{\\text{data},c} - N_{\\text{other},c}}{N_{\\text{top},c}}}\n",
    "$$\n",
    "\n",
    "### 3. Signal Strength (POI)\n",
    "$$\n",
    "\\boxed{\\mu_c = \\frac{N_{\\text{obs},c} - N_{\\text{bkg},c}}{N_{\\text{signal},c}}}\n",
    "$$\n",
    "\n",
    "### 4. Combined Signal Strength\n",
    "$$\n",
    "\\boxed{\\mu_{\\text{combined}} = \\frac{\\sum_c \\mu_c / \\sigma_{\\mu,c}^2}{\\sum_c 1 / \\sigma_{\\mu,c}^2} \\pm \\frac{1}{\\sqrt{\\sum_c 1 / \\sigma_{\\mu,c}^2}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ccdce-200a-4ba4-b426-75ef484e6ca2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35ad680-8028-447b-ad24-2244ebd0d72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done!\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import uproot\n",
    "    import awkward\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import time\n",
    "    import dask\n",
    "    from dask import delayed, compute\n",
    "    from dask.distributed import Client, progress\n",
    "    from pathlib import Path\n",
    "    from tqdm import tqdm \n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    print(\"All imports done!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9063709-41f1-4521-a46e-a9f1033953de",
   "metadata": {},
   "source": [
    "# Step 1: Find sum of gen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b658502-dee6-45e8-9417-013e8e6b7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/DYJetsToLL_M-50_TuneCP5_13TeV-madgraphMLM-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/40000/751D5714-5507-6343-818A-5DB7797D6632.root\"\n",
    "\n",
    "# with uproot.open(url) as f:\n",
    "#     tree = f['Events']\n",
    "#     for branches in tree.keys():\n",
    "#         if \"weight\" in branches.lower():\n",
    "#             print(f'-{branches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55e9bb-0c23-419b-a2e5-4d840f5f704d",
   "metadata": {},
   "source": [
    "These are all the branches in the file with \"weight\", we are going to use the `genWeight` branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4425bc3c-364c-4115-9bac-1782271ece29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The working directory is: \n",
      "/home/cms-jovyan/H-to-WW-NanoAOD-analysis/Datasets/MC_samples\n",
      "\n",
      "The files here are\n",
      "-VG.txt\n",
      "-Higgs.txt\n",
      "-WW.txt\n",
      "-Fakes.txt\n",
      "-VZ.txt\n",
      "-DYtoLL.txt\n",
      "-ggWW.txt\n",
      "-Top.txt\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"/home/cms-jovyan/H-to-WW-NanoAOD-analysis/Datasets/MC_samples\"\n",
    "os.chdir(BASE_PATH)\n",
    "print(f\"The working directory is: \\n{os.getcwd()}\")\n",
    "\n",
    "print(f\"\\nThe files here are\")\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\"txt\"):\n",
    "        print(f\"-{file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05f924d-bb99-4485-b05f-3e5a7b447dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8 samples in parallel with 8 workers...\n",
      "\n",
      " Starting: DYtoLL\n",
      " Starting: ggWW\n",
      " Starting: Fakes\n",
      " Starting: WW\n",
      " Starting: VZ\n",
      " Starting: Higgs\n",
      " Starting: Top\n",
      " Starting: VG\n",
      " Completed: WW (7 files)\n",
      " Completed: Higgs (40 files)\n",
      "  ggWW: processed 50/112 files\n",
      " Completed: VG (32 files)\n",
      "  VZ: processed 50/73 files\n",
      "  Top: processed 50/197 files\n",
      " Completed: VZ (73 files)\n",
      "  DYtoLL: processed 50/61 files\n",
      "  Fakes: processed 50/206 files\n",
      "  ggWW: processed 100/112 files\n",
      " Completed: DYtoLL (61 files)\n",
      " Completed: ggWW (112 files)\n",
      "  Top: processed 100/197 files\n",
      "  Fakes: processed 100/206 files\n",
      "  Top: processed 150/197 files\n",
      "  Fakes: processed 150/206 files\n",
      " Completed: Top (197 files)\n",
      "  Fakes: processed 200/206 files\n",
      " Completed: Fakes (206 files)\n",
      "\n",
      "============================================================\n",
      "RESULTS BY SAMPLE:\n",
      "============================================================\n",
      "VG             : weight=  3109819392.00, events=  34915878, files=32\n",
      "Higgs          : weight=    63281828.00, events=   2946000, files=40\n",
      "WW             : weight=    32147096.00, events=   2900000, files=7\n",
      "Fakes          : weight=9740958564352.00, events= 225680227, files=206\n",
      "VZ             : weight=   134985184.00, events=  15551954, files=73\n",
      "DYtoLL         : weight=    82448512.00, events=  82448537, files=61\n",
      "ggWW           : weight=    17662000.00, events=  17662000, files=112\n",
      "Top            : weight= 11433399296.00, events= 137367000, files=197\n",
      "Total execution time: 908.43 seconds (15.14 minutes)\n"
     ]
    }
   ],
   "source": [
    "def process_sample(txt_file):\n",
    "    \n",
    "    sample_name = Path(txt_file).stem\n",
    "    print(f\" Starting: {sample_name}\")\n",
    "    \n",
    "    \n",
    "    # Read list of ROOT files\n",
    "    with open(txt_file) as f:\n",
    "        root_files = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    sum_gen_weight = 0\n",
    "    n_events = 0\n",
    "    n_files_processed = 0\n",
    "\n",
    "    for i, file in enumerate(root_files, 1):\n",
    "        try:\n",
    "            with uproot.open(file) as f:\n",
    "                if \"Events\" not in f:\n",
    "                    continue\n",
    "                \n",
    "                tree = f[\"Events\"]\n",
    "\n",
    "                for data in tree.iterate(\"genWeight\", step_size=\"100MB\"):\n",
    "                    weights = data[\"genWeight\"].to_numpy()\n",
    "                    sum_gen_weight += np.sum(weights)\n",
    "                    n_events += len(weights)\n",
    "            \n",
    "            n_files_processed += 1\n",
    "            if i % 50 == 0:  # Print progress every 50 files\n",
    "                print(f\"  {sample_name}: processed {i}/{len(root_files)} files\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error in {sample_name}/{file}: {e}\")\n",
    "\n",
    "    print(f\" Completed: {sample_name} ({n_files_processed} files)\")\n",
    "    return sample_name, sum_gen_weight, n_events, n_files_processed\n",
    "\n",
    "\n",
    "# List of samples\n",
    "txt_files = [\n",
    "    \"VG.txt\", \"Higgs.txt\", \"WW.txt\", 63281828Fakes.txt\",\n",
    "    \"VZ.txt\", \"DYtoLL.txt\", \"ggWW.txt\", \"Top.txt\"\n",
    "]\n",
    "# txt_files = [\"VG.txt\"]\n",
    "\n",
    "print(f\"Processing {len(txt_files)} samples in parallel with 8 workers...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "tasks = [dask.delayed(process_sample)(txt_file) for txt_file in txt_files]\n",
    "\n",
    "\n",
    "with dask.config.set(scheduler='threads', num_workers=8):\n",
    "    results = dask.compute(*tasks)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS BY SAMPLE:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_weight = 0\n",
    "total_events = 0\n",
    "\n",
    "for sample_name, sum_weight, n_events, n_files in results:\n",
    "    print(f\"{sample_name:15s}: weight={sum_weight:15.2f}, events={n_events:10d}, files={n_files}\")\n",
    "    total_weight += sum_weight\n",
    "    total_events += n_events\n",
    "\n",
    "\n",
    "print(f\"Total execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e38439-85cc-4e99-9adb-172b1953c13e",
   "metadata": {},
   "source": [
    "### Saving the result as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065836e6-b675-4040-9832-f0241a4520a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS BY SAMPLE:\n",
      "============================================================\n",
      "saved to csv file: genweight_results.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(results, columns=['Sample', 'GenWeight_Sum', 'N_Events', 'N_Files'])\n",
    "\n",
    "\n",
    "total_rows = pd.DataFrame({\n",
    "    'Sample': ['TOTAL'],\n",
    "    'GenWeight_Sum': [df['GenWeight_Sum'].sum()],      \n",
    "    'N_Events': [df['N_Events'].sum()],                \n",
    "    'N_Files': [df['N_Files'].sum()]                   \n",
    "})\n",
    "\n",
    "df = pd.concat([df, total_rows], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = f\"/home/cms-jovyan/H-to-WW-NanoAOD-analysis/notebooks/genweight_results.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS BY SAMPLE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"saved to csv file: genweight_results.csv\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9355e78-24d1-4160-84c0-d4aeea63e600",
   "metadata": {},
   "source": [
    "# Step 2: Getting Cross-section of the processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b02aca-6ef7-4c27-8ee8-ff153b3054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross-sections for the processes are:\n",
    "\n",
    "sample_info = {\n",
    "                \"Higgs\": {\"xsec\": 1.0315, \"genWeight\": 63_281_828.0},\n",
    "                \"DYtoLL\": {\"xsec\": 6189.39, \"genWeight\": 82_448_512.0},\n",
    "                \"Top\": {\"xsec\": 232.58, \"genWeight\": 11_433_399_296.0},\n",
    "                \"Fakes\": {\"xsec\": 61_891.05, \"genWeight\": 9_740_958_564_352.0},\n",
    "                \"VZ\": {\"xsec\": 26.54765, \"genWeight\": 134_985_184.0},\n",
    "                \"ggWW\": {\"xsec\": 5.7483, \"genWeight\": 17_662_000.0},\n",
    "                \"WW\": {\"xsec\": 12.178, \"genWeight\": 32_147_096.0},\n",
    "                \"VG\": {\"xsec\": 464.101, \"genWeight\": 3_109_819_392.0}\n",
    "}\n",
    "\n",
    "Luminosity_pb = 38_250 # in pb^-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a828cfa1-ea62-4685-a3f2-d2b703b94977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_factors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a98e733-1ba9-448f-b4f3-f043b73b7c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Higgs and {'xsec': 1.0315, 'genWeight': 63281828.0}\n",
      "-DYtoLL and {'xsec': 6189.39, 'genWeight': 82448512.0}\n",
      "-Top and {'xsec': 232.58, 'genWeight': 11433399296.0}\n",
      "-Fakes and {'xsec': 61891.05, 'genWeight': 9740958564352.0}\n",
      "-VZ and {'xsec': 26.54765, 'genWeight': 134985184.0}\n",
      "-ggWW and {'xsec': 5.7483, 'genWeight': 17662000.0}\n",
      "-WW and {'xsec': 12.178, 'genWeight': 32147096.0}\n",
      "-VG and {'xsec': 464.101, 'genWeight': 3109819392.0}\n"
     ]
    }
   ],
   "source": [
    "for sample, info in sample_info.items():\n",
    "    print(f\"-{sample} and {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e2a0680-2f6d-48d9-9d1c-65f1bbe368f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scale_factors(sample_info, Luminosity_pb):\n",
    "    scale_factors = {}\n",
    "    for sample, info in sample_info.items():\n",
    "        sf = (info['xsec']*Luminosity_pb)/info['genWeight']\n",
    "        scale_factors[sample] = sf\n",
    "    return scale_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c820192d-41dd-4d2a-882a-55ffd2c1eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dict = get_scale_factors(sample_info, Luminosity_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b13c66a-9370-4f0f-bea1-c9527a65018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higgs: 0.000623479\n",
      "DYtoLL: 2.871418316\n",
      "Top: 0.000778087\n",
      "Fakes: 0.000243029\n",
      "VZ: 0.007522660\n",
      "ggWW: 0.012448900\n",
      "WW: 0.014489909\n",
      "VG: 0.005708326\n"
     ]
    }
   ],
   "source": [
    "for sample, sf in sf_dict.items():\n",
    "    print(f\"{sample}: {sf:.9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc34493-b73d-4a41-ba14-52ed0a235b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
