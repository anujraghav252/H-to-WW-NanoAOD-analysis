{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394ac4c3-8b73-4741-bae0-7224bcd168dc",
   "metadata": {},
   "source": [
    "# **Welcome to the Full Analysis**\n",
    "\n",
    "Welcome! This notebook contains our complete, start-to-finish analysis pipeline. It is designed to act as a walkthrough, guiding you step-by-step through the physics logic, data processing, and visualization.\n",
    "\n",
    "If you are already familiar with the workflow and just want a streamlined, ready-to-run version without all the detailed explanations, head over to **`Run_analysis/Run_analysis.ipynb`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb73848-9fec-4e22-a792-48f7ad721a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Updated: 24 February 2026, 20:41 IST\n"
     ]
    }
   ],
   "source": [
    "# A time stamp to see the last update time \n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "ist = pytz.timezone('Asia/Kolkata')\n",
    "print(f\"Last Updated: {datetime.now(ist).strftime('%d %B %Y, %H:%M')} IST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e42a7-98c4-403e-9815-00befd42b9bb",
   "metadata": {},
   "source": [
    "# **Imports**\n",
    "\n",
    "Let's bring in all the required Python libraries for our analysis. To keep things clean, we have grouped them into specific categories so you can easily see what each tool is used for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b69d96b-e69d-45a1-8644-50cb190a0270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Handing file paths, memory management (garbage collection), and reading configurations\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Libraries for reading ROOT files and managing jagged arrays\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries, including mplhep for standard CMS styles\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as mpatches\n",
    "import mplhep as hep\n",
    "\n",
    "# Physics Kinematics & Histograms\n",
    "# Tools for 4-vector math, filling multi-dimensional bins, and progress tracking\n",
    "import vector\n",
    "import hist\n",
    "from hist import Hist\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Distributed Computing\n",
    "# Dask tools to scale our event loop across multiple cluster workers\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster, progress, as_completed\n",
    "# Register vector behavior so it can process our awkward arrays of particles\n",
    "vector.register_awkward()\n",
    "\n",
    "print(\" All imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65af62-94be-4eaa-ad4a-11574b9afc7d",
   "metadata": {},
   "source": [
    "## **Setting up the Dask Client**\n",
    "\n",
    "With our imports ready, the next step is to initialize the Dask client. We are connecting to `localhost:8786` here, but feel free to swap this out with your own scheduler address depending on the machine or cluster you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7fc7a28-9d06-42a7-8fa1-19bf1f71bf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-0aecbba7-1193-11f1-913a-fa3756322e2d</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Direct</td>\n",
       "            <td style=\"text-align: left;\"></td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/user/anujraghav.physics@gmail.com/proxy/8787/status\" target=\"_blank\">/user/anujraghav.physics@gmail.com/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/user/anujraghav.physics@gmail.com/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Scheduler Info</h3></summary>\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-c9debe45-19dc-485a-aa31-a75629c85dc7</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tls://192.168.197.201:8786\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/user/anujraghav.physics@gmail.com/proxy/8787/status\" target=\"_blank\">/user/anujraghav.physics@gmail.com/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tls://192.168.197.201:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(\"tls://localhost:8786\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd6b55b-fc47-4dc5-9ede-0baa7a3615d6",
   "metadata": {},
   "source": [
    "## Directories and File Paths\n",
    "\n",
    "This section sets up all the file paths required for the analysis. If you have cloned this repository, the folder structure is already set up for you, you just need to populate the directories with your dataset files. If you are writing this from scratch, make sure to point these variables to the correct locations on your machine.\n",
    "\n",
    "Defining these dynamic paths clearly upfront ensures that you can run this notebook smoothly from anywhere without running into \"file not found\" errors later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6491522-1860-420b-9f68-62e04daf5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE PATHS\n",
    "HOME_DIR = Path(os.environ.get(\"HOME\", \"/home/cms-jovyan\"))\n",
    "PROJECT_NAME = \"H-to-WW-NanoAOD-analysis\"\n",
    "\n",
    "# DERIVED PATHS\n",
    "PROJECT_DIR = HOME_DIR / PROJECT_NAME\n",
    "DATASETS_DIR = PROJECT_DIR / \"Datasets\"\n",
    "DATA_DIR = DATASETS_DIR / \"DATA\"\n",
    "MC_DIR = DATASETS_DIR / \"MC_samples\"\n",
    "AUX_DIR = PROJECT_DIR / \"Auxillary_files\"\n",
    "OUTPUT_DIR = Path.cwd() / \"Outputs\"\n",
    "PLOTS_DIR = OUTPUT_DIR / \"Plots\"\n",
    " \n",
    "# Files\n",
    "GOLDEN_JSON_PATH = AUX_DIR / \"Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\"\n",
    "LEPTON_ID_SF = AUX_DIR / \"lepID_lookup.txt\"\n",
    "\n",
    "# RUN PERIODS\n",
    "RUN_PERIODS_2016 = {\n",
    "    'Run2016G': {'run_min': 278820, 'run_max': 280385},\n",
    "    'Run2016H': {'run_min': 280919, 'run_max': 284044}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66628fff-a258-4e3e-8e94-639cedecc2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cms-jovyan/H-to-WW-NanoAOD-analysis/test/Outputs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac86e2-f34c-40e9-98eb-877fd68a72f1",
   "metadata": {},
   "source": [
    "## Setting Up Configurations and Labels\n",
    "\n",
    "Next, let's set up the dictionaries and lists that will guide our analysis. \n",
    "\n",
    "Instead of hard-coding colors and labels later on, we define them all right here. We start by assigning a standard name, color, and stacking order to each of our Monte Carlo backgrounds and signal. Then, we map out the exact sequence of our cutflow stages so we can accurately track how many events survive our physics cuts. Finally, we include a list of LaTeX-formatted labels so that when we generate our plots at the end, the axes are already perfectly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1ae7ed-de6c-434a-a36b-adbe0615ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE DEFINITIONS\n",
    "\n",
    "# Sample mapping \n",
    "SAMPLE_MAPPING = {\n",
    "    'data': 'Data',\n",
    "    'higgs': 'ggH_HWW',\n",
    "    'dytoll': 'DY_to_Tau_Tau',\n",
    "    'top': 'Top_antitop',\n",
    "    'fakes': 'Fakes',\n",
    "    'vz': 'Diboson',\n",
    "    'ggww': 'ggWW',\n",
    "    'ww': 'WW',\n",
    "    'vg': 'VG'\n",
    "}\n",
    "\n",
    "# Sample properties (color, signal flag, stacking order)\n",
    "SAMPLES = {\n",
    "    \"Fakes\":          {\"color\": \"#B3B3B3\", \"is_signal\": False, \"stack_order\": 1},\n",
    "    \"VG\":             {\"color\": \"#FFCC00\", \"is_signal\": False, \"stack_order\": 2},\n",
    "    \"Diboson\":        {\"color\": \"#A6CEE3\", \"is_signal\": False, \"stack_order\": 3},\n",
    "    \"DY_to_Tau_Tau\":  {\"color\": \"#33A02C\", \"is_signal\": False, \"stack_order\": 4},\n",
    "    \"Top_antitop\":    {\"color\": \"#FF7F00\", \"is_signal\": False, \"stack_order\": 5},\n",
    "    \"ggWW\":           {\"color\": \"#6BAED6\", \"is_signal\": False, \"stack_order\": 6},\n",
    "    \"WW\":             {\"color\": \"#1F78B4\", \"is_signal\": False, \"stack_order\": 7},\n",
    "    \"ggH_HWW\":        {\"color\": \"#E41A1C\", \"is_signal\": True,  \"stack_order\": 8},\n",
    "    \"Data\":           {\"color\": \"#000000\", \"is_signal\": False, \"stack_order\": -1},\n",
    "}\n",
    "\n",
    "# Derived helper dictionaries\n",
    "colour = {name: props[\"color\"] for name, props in SAMPLES.items()}\n",
    "stack_order = {name: props[\"stack_order\"] for name, props in SAMPLES.items() if props[\"stack_order\"] >= 0}\n",
    "\n",
    "# Sample order for printing (cutflow table)\n",
    "sample_order = [\n",
    "    'Data',\n",
    "    'ggH_HWW',\n",
    "    'WW',\n",
    "    'Top_antitop',\n",
    "    'DY_to_Tau_Tau',\n",
    "    'Fakes',\n",
    "    'ggWW',\n",
    "    'Diboson',\n",
    "    'VG',\n",
    "]\n",
    "\n",
    "dict_for_xsec_mapping = {\n",
    "    \"DYJetsToLL\": \"DYJetsToLL_M-50\",\n",
    "    \"TTTo2L2Nu\": \"TTTo2L2Nu\",\n",
    "    \"ST_t-channel_top\": \"ST_t-channel_top\",\n",
    "    \"ST_t-channel_antitop\": \"ST_t-channel_antitop\",\n",
    "    \"ST_tW_antitop\": \"ST_tW_antitop\",\n",
    "    \"ST_tW_top\": \"ST_tW_top\",\n",
    "    \"ST_s-channel\": \"ST_s-channel\",\n",
    "    \"WJetsToLNu\": \"WJetsToLNu\",\n",
    "    \"TTToSemiLeptonic\": \"TTToSemiLeptonic\",\n",
    "    \"ZGToLLG\": \"ZGToLLG\",\n",
    "    \"WGToLNuG\": \"WGToLNuG\",\n",
    "    \"WZTo3LNu\": \"WZTo3LNu\",\n",
    "    \"WZTo2Q2L\": \"WZTo2Q2L\",\n",
    "    \"ZZ\": \"ZZ\",\n",
    "    \"GluGluToWW\": \"GluGluToWW\",\n",
    "    \"WWTo2L2Nu\": \"WWTo2L2Nu\",\n",
    "    \"GluGluHToWW\": \"Higgs\",\n",
    "    \"Higgs\": \"Higgs\",\n",
    "}\n",
    "\n",
    "# CUTFLOW & ANALYSIS STAGES\n",
    "\n",
    "# All cutflow stages\n",
    "cutflow_stages = [\n",
    "    'total', 'after_json', 'e_mu_preselection', 'global_cuts',\n",
    "    '0jet', '1jet', '2jet',\n",
    "    'SR_0jet', 'SR_1jet', 'SR_2jet',\n",
    "    'CR_top_0jet', 'CR_top_1jet', 'CR_top_2jet',\n",
    "    'CR_tau_0jet', 'CR_tau_1jet', 'CR_tau_2jet', \n",
    "    'CR_SS_0jet',\n",
    "    'CR_WW_0jet', 'CR_WW_1jet', 'CR_WW_2jet'\n",
    "]\n",
    "\n",
    "# Stage names for histogram initialization\n",
    "stage_names = [\n",
    "    'before_cuts', 'global', '0jet', '1jet', '2jet',\n",
    "    'SR_0jet', 'SR_1jet', 'SR_2jet',\n",
    "    'CR_top_0jet', 'CR_top_1jet', 'CR_top_2jet',\n",
    "    'CR_tau_0jet', 'CR_tau_1jet', 'CR_tau_2jet', \n",
    "    'CR_SS_0jet',\n",
    "    'CR_WW_0jet', 'CR_WW_1jet', 'CR_WW_2jet'\n",
    "]\n",
    "\n",
    "# Stage info: (internal_name, display_name) for cutflow table\n",
    "stage_info = [\n",
    "    ('total', 'Total (Raw)'),\n",
    "    ('e_mu_preselection', 'e-Î¼ Preselect'),\n",
    "    ('global_cuts', 'Global Cuts'),\n",
    "    ('0jet', '0-jet'),\n",
    "    ('1jet', '1-jet'),\n",
    "    ('2jet', '2-jet'),\n",
    "    ('SR_0jet', 'SR 0j'),\n",
    "    ('SR_1jet', 'SR 1j'),\n",
    "    ('SR_2jet', 'SR 2j'),\n",
    "    ('CR_top_0jet', 'CR Top 0j'),\n",
    "    ('CR_top_1jet', 'CR Top 1j'),\n",
    "    ('CR_top_2jet', 'CR Top 2j'),\n",
    "    ('CR_tau_0jet', 'CR Tau 0j'),\n",
    "    ('CR_tau_1jet', 'CR Tau 1j'),\n",
    "    ('CR_tau_2jet', 'CR Tau 2j'),\n",
    "    ('CR_SS_0jet', 'CR SS 0j'),\n",
    "    ('CR_WW_0jet', 'CR WW 0j'),  \n",
    "    ('CR_WW_1jet', 'CR WW 1j'),  \n",
    "    ('CR_WW_2jet', 'CR WW 2j')  \n",
    "]\n",
    "\n",
    "# HISTOGRAM VARIABLE LABELS\n",
    "\n",
    "VAR_LABELS = {\n",
    "    'mass': r'$m_{e\\mu}$ [GeV]',\n",
    "    'met': r'$E_{\\mathrm{T}}^{\\mathrm{miss}}$ [GeV]',\n",
    "    'ptll': r'$p_{\\mathrm{T}}^{\\ell\\ell}$ [GeV]',\n",
    "    'dphi': r'$\\Delta\\phi(e,\\mu)$',\n",
    "    'mt_higgs': r'$m_{\\mathrm{T}}^{H}$ [GeV]',\n",
    "    'mt_l2_met': r'$m_{\\mathrm{T}}(\\ell_2, E_{\\mathrm{T}}^{\\mathrm{miss}})$',\n",
    "    'mjj': r'$m_{jj}$ [GeV]',\n",
    "    'leading_pt': r'$p_{\\mathrm{T}}^{\\mathrm{lead}}$ [GeV]',\n",
    "    'subleading_pt': r'$p_{\\mathrm{T}}^{\\mathrm{sub}}$ [GeV]',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b2294-c01f-4072-a0b1-c8d88495191d",
   "metadata": {},
   "source": [
    "## Loading the Datasets\n",
    "\n",
    "Instead of downloading massive ROOT files locally, we stream the data directly via URLs. The text files containing these URLs for both our collision Data and Monte Carlo (MC) samples are located in the **`Datasets/DATA`** and **`Datasets/MC_samples`** directories. (There is also a **README** in that folder if you want more details on the dataset specifics).\n",
    "\n",
    "This next code block simply reads through those text files, extracts the URLs, and organizes them so our processor can read them.\n",
    "\n",
    "> ***Tip**: At the bottom of the cell, you will notice a commented-out line that restricts the list to just one file per sample. This is very useful for running a quick local test to ensure your code works before sending the full job to the cluster.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af86eb87-48fe-42b0-8137-1aa4f0ffc2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FILES TO PROCESS\n",
      "======================================================================\n",
      "Data                :    1 files\n",
      "VG                  :    1 files\n",
      "ggH_HWW             :    1 files\n",
      "WW                  :    1 files\n",
      "Fakes               :    1 files\n",
      "Diboson             :    1 files\n",
      "DY_to_Tau_Tau       :    1 files\n",
      "ggWW                :    1 files\n",
      "Top_antitop         :    1 files\n",
      "______________________________________________________________________\n",
      "TOTAL               :    9 files\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def load_urls_from_file(filepath, max_files=None):\n",
    "    \"\"\"Load XRootD URLs from text file\"\"\"\n",
    "    urls = []\n",
    "    if not os.path.exists(filepath):\n",
    "        return urls\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and line.startswith('root://'):\n",
    "                urls.append(line)\n",
    "                if max_files and len(urls) >= max_files:\n",
    "                    break\n",
    "    return urls\n",
    "\n",
    "def load_all_files(data_dir, mc_dir, max_per_sample=None):\n",
    "    \"\"\"Load all file URLs from directories\"\"\"\n",
    "    \n",
    "    files_dict = {}\n",
    "    \n",
    "    for directory in [data_dir, mc_dir]:\n",
    "        if not os.path.exists(directory):\n",
    "            continue\n",
    "        for filename in os.listdir(directory):\n",
    "            if not filename.endswith(('.txt')):\n",
    "                continue\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            filename_lower = filename.lower().replace('.txt', '')\n",
    "            # Find label\n",
    "            label = None\n",
    "            for pattern, sample_label in SAMPLE_MAPPING.items():\n",
    "                if pattern in filename_lower:\n",
    "                    label = sample_label\n",
    "                    break          \n",
    "            if not label:\n",
    "                print(f\"   Unknown file: {filename} - skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Load URLs\n",
    "            urls = load_urls_from_file(filepath, max_per_sample)\n",
    "            \n",
    "            if urls:\n",
    "                if label in files_dict:\n",
    "                    files_dict[label].extend(urls)\n",
    "                else:\n",
    "                    files_dict[label] = urls\n",
    "    \n",
    "    return files_dict\n",
    "\n",
    "files = load_all_files(DATA_DIR, MC_DIR, max_per_sample=1)  # TESTING\n",
    "# files = load_all_files(DATA_DIR, MC_DIR)  # FULL \n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILES TO PROCESS\")\n",
    "print(\"=\"*70)\n",
    "total = 0\n",
    "for label, urls in files.items():\n",
    "    print(f\"{label:20s}: {len(urls):4d} files\")\n",
    "    total += len(urls)\n",
    "print(\"_\"*70)\n",
    "print(f\"{'TOTAL':20s}: {total:4d} files\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a403b4-0100-4e47-aa6b-4bcce437ebc6",
   "metadata": {},
   "source": [
    "## Filtering Validated Runs\n",
    "\n",
    "Not all the data collected by the detector is suitable for physics analysis. Sometimes certain subdetectors are offline or experiencing issues. To ensure we only process high-quality data, we use a JSON file (often called the \"Golden JSON\") to filter our dataset. This acts as a master list, keeping only the certified runs and luminosity blocks.\n",
    "\n",
    "If you want to dive deeper into how this works, you can read more about [validated runs](https://cms-opendata-guide.web.cern.ch/analysis/selection/validatedRuns/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceda2da0-f414-4bd3-9cfe-9b3abefd64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_golden_json(json_input, run_periods=None):\n",
    "    \"\"\"\n",
    "    Load golden JSON from either a file path (str) or a dict.\n",
    "    \"\"\"\n",
    "    if isinstance(json_input, str):\n",
    "        with open(json_input, 'r') as f:\n",
    "            golden_json = json.load(f)\n",
    "    elif isinstance(json_input, dict):\n",
    "        golden_json = json_input\n",
    "    else:\n",
    "        raise TypeError(f\"Expected str or dict, got {type(json_input)}\")\n",
    "    \n",
    "    valid_lumis = {}\n",
    "    for run_str, lumi_ranges in golden_json.items():\n",
    "        run = int(run_str)\n",
    "        \n",
    "        # Filter by run periods \n",
    "        if run_periods is not None: \n",
    "            in_period = any(\n",
    "                period['run_min'] <= run <= period['run_max']\n",
    "                for period in run_periods.values()\n",
    "            )\n",
    "            if not in_period:\n",
    "                continue\n",
    "        \n",
    "        valid_lumis[run] = [tuple(lr) for lr in lumi_ranges]\n",
    "    \n",
    "    return valid_lumis\n",
    "\n",
    "def apply_json_mask(arrays, json_input, run_periods=None):\n",
    "\n",
    "    valid_lumis = load_golden_json(json_input, run_periods)    \n",
    "    runs = ak.to_numpy(arrays.run)\n",
    "    lumis = ak.to_numpy(arrays.luminosityBlock)\n",
    "    mask = np. zeros(len(runs), dtype=bool)\n",
    "    \n",
    "    for run, lumi_ranges in valid_lumis.items():\n",
    "        run_mask = (runs == run)\n",
    "        if not np.any(run_mask):\n",
    "            continue\n",
    "        # Check lumi sections \n",
    "        run_lumis = lumis[run_mask]\n",
    "        run_lumi_mask = np.zeros(len(run_lumis), dtype=bool)\n",
    "        for lumi_start, lumi_end in lumi_ranges: \n",
    "            run_lumi_mask |= (run_lumis >= lumi_start) & (run_lumis <= lumi_end)\n",
    "        mask[run_mask] = run_lumi_mask\n",
    "    \n",
    "    return ak.Array(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e71da0-f21f-4324-b4ac-4daa7c610c82",
   "metadata": {},
   "source": [
    "## Loading Only What We Need (Branches)\n",
    "\n",
    "ROOT files contain hundreds of variables (called branches), but we definitely do not need to load all of them into memory. By using `uproot`, we can specify exactly which branches we care about, saving us a massive amount of RAM and processing time.\n",
    "\n",
    "> **Wait, New to ROOT terminology?**\\\n",
    "> If words like \"Trees\" and \"Branches\" sound foreign to you, check out [the official ROOT manual](https://root.cern/manual/trees/) for a quick primer on how these data files are structured.\n",
    "\n",
    "\n",
    "For this analysis, we are primarily interested in the kinematics of electrons, muons, and a few jet-related properties. \n",
    "* **For real Data:** We also load the `run` and `luminosityBlock` branches to apply the JSON validation we just talked about.\n",
    "* **For Monte Carlo (MC):** We load the `genWeight` branch to handle theoretical event scaling (we will cover this in detail a bit later!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e692653-b784-46df-906c-31ac19dc8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_events(file_url, batch_size= 1_000_000, timeout=600, max_retries=3, retry_wait=10, is_data = False):\n",
    "\n",
    "    columns = [\n",
    "        \"Electron_pt\", \"Electron_eta\", \"Electron_phi\", \"Electron_mass\", \n",
    "        \"Electron_mvaFall17V2Iso_WP90\", \"Electron_charge\",\n",
    "        \n",
    "        \"Muon_pt\", \"Muon_eta\", \"Muon_phi\", \"Muon_mass\", \n",
    "        \"Muon_tightId\", \"Muon_charge\", \"Muon_pfRelIso04_all\",\n",
    "        \"PuppiMET_pt\", \"PuppiMET_phi\",\n",
    "        \n",
    "        \"Jet_pt\", \"Jet_eta\", \"Jet_phi\", \"Jet_mass\",\n",
    "        \"Jet_btagDeepFlavB\", \"nJet\", \"Jet_jetId\", \"Jet_puId\",\n",
    "    ]\n",
    "\n",
    "    if is_data:\n",
    "        columns.extend([\"run\",\"luminosityBlock\"])\n",
    "    else: columns.append(\"genWeight\")\n",
    "        \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            \n",
    "            with uproot.open(file_url, timeout=timeout) as f:\n",
    "                tree = f['Events']\n",
    "                \n",
    "                \n",
    "                for arrays in tree.iterate(columns, step_size=batch_size, library=\"ak\"):\n",
    "                    yield arrays\n",
    "                \n",
    "                return\n",
    "                \n",
    "        except (TimeoutError, OSError, IOError, ConnectionError) as e:\n",
    "            error_type = type(e).__name__\n",
    "            file_name = file_url.split('/')[-1]\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"      {error_type} on {file_name}\")\n",
    "                print(f\"       Retry {attempt+1}/{max_retries-1} in {retry_wait}s...\")\n",
    "                time.sleep(retry_wait)\n",
    "            else:\n",
    "                print(f\"     FAILED after {max_retries} attempts: {file_name}\")\n",
    "                print(f\"       Error: {str(e)[:100]}\")\n",
    "                raise\n",
    "                \n",
    "        except Exception as e:\n",
    "            \n",
    "            file_name = file_url.split('/')[-1]\n",
    "            print(f\"     Unexpected error on {file_name}: {str(e)[:100]}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80b541-963c-4b8b-9a5a-c10e2b872ecb",
   "metadata": {},
   "source": [
    "# **Event Selection**\n",
    "\n",
    "Now that we are done with the  setup, we can get to the real physics. \n",
    "\n",
    "Let's take a look at the specific decay channel we are analyzing:\n",
    "\n",
    "$$H \\to WW \\to e\\nu_e + \\mu\\nu_\\mu$$\n",
    "\n",
    "\n",
    "Our target final state consists of an *electron*, a *muon*, and two *neutrinos*. In our dataset, this translates to an electron, a muon, and missing energy, because we cannot detect neutrinos directly with the detector. **Why?** \n",
    "\n",
    "To isolate this specific Higgs signal from the massive background of other collision events, we will now apply a series of targeted physics cuts.\\\n",
    "We will divide all the cuts into three categories:\n",
    "- Pre-selection\n",
    "- Global cuts\n",
    "- Signal and Control Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093236e-6999-44f0-8ebf-67602167b3a1",
   "metadata": {},
   "source": [
    "## Pre-Selection: Selecting tight leptons\n",
    "\n",
    "Let's kick things off by setting up our initial pre-selection cuts. \n",
    "\n",
    "The very first step is to isolate our \"tight\" leptons-which, for this analysis, means our electrons and muons. \n",
    "\n",
    "> **What makes a lepton \"tight\"?**\n",
    "> In particle physics, a tight lepton is simply one that passes a very strict set of quality and isolation criteria. This helps us guarantee that the particle we are looking at is genuinely a high-quality electron or muon, rather than a fake signal or background noise.\n",
    "\n",
    "To handle this data efficiently, we use the `ak.zip` function from Awkward Array. This allows us to bundle all the relevant particle properties into a single, neatly structured array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e90cf2c-6119-4276-8115-bc2135da7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tight_leptons(arrays):\n",
    "    \"\"\"Apply tight ID and isolation cuts to leptons\"\"\"\n",
    "    # Define selection masks\n",
    "    tight_electron_mask = arrays.Electron_mvaFall17V2Iso_WP90 == 1\n",
    "    tight_muon_mask = (arrays.Muon_tightId == 1) & (arrays.Muon_pfRelIso04_all < 0.15)\n",
    "    \n",
    "    # Create structured arrays for selected leptons\n",
    "    tight_electrons = ak.zip({\n",
    "        \"pt\": arrays.Electron_pt[tight_electron_mask],\n",
    "        \"eta\": arrays.Electron_eta[tight_electron_mask],\n",
    "        \"phi\": arrays.Electron_phi[tight_electron_mask],\n",
    "        \"mass\": arrays.Electron_mass[tight_electron_mask],\n",
    "        \"charge\": arrays.Electron_charge[tight_electron_mask],\n",
    "        \"flavor\": ak.ones_like(arrays.Electron_pt[tight_electron_mask]) * 11\n",
    "    })\n",
    "    \n",
    "    tight_muons = ak.zip({\n",
    "        \"pt\": arrays.Muon_pt[tight_muon_mask],\n",
    "        \"eta\": arrays.Muon_eta[tight_muon_mask],\n",
    "        \"phi\": arrays.Muon_phi[tight_muon_mask],\n",
    "        \"mass\": arrays.Muon_mass[tight_muon_mask],\n",
    "        \"charge\": arrays.Muon_charge[tight_muon_mask],\n",
    "        \"flavor\": ak.ones_like(arrays.Muon_pt[tight_muon_mask]) * 13\n",
    "    })\n",
    "    \n",
    "    # Combine into single collection\n",
    "    tight_leptons = ak.concatenate([tight_electrons, tight_muons], axis=1)\n",
    "    \n",
    "    return tight_leptons, tight_electron_mask, tight_muon_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e79549-b977-4819-a9f5-2ab984747f44",
   "metadata": {},
   "source": [
    "## Pre-Selection: Finding Our Lepton Pair\n",
    "\n",
    "Now that we have our collection of \"tight\" leptons, we need to filter them down to the specific electron and muon candidates that match our signal. \n",
    "\n",
    "First, we require exactly two leptons per event. Because the W bosons in our signal have opposite charges ($W^+W^-$), we specifically look for an electron and a muon with opposite electrical charges. \n",
    "\n",
    "Next, we apply a few essential kinematic cuts:\n",
    "* **Transverse Momentum ($p_T$):** The leptons must pass a specific $p_T$ threshold. This ensures we are looking at real, high-energy physics events rather than low-energy background noise.\n",
    "* **Pseudorapidity ($|\\eta|$):** We restrict the particles to the geometric acceptance range of the detector. If a particle is produced at an angle too close to the beamline (a very high $|\\eta|$), the CMS detector cannot measure it accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31924a6c-055d-457a-8751-fb1a4e5cb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_e_mu_events(tight_leptons, met_arrays, leading_pt_cut=25, subleading_pt_cut=13):\n",
    "    \"\"\"Select events with exactly 1 electron and 1 muon\"\"\"\n",
    "    # Sort by pT\n",
    "    sorted_leptons = tight_leptons[ak.argsort(tight_leptons.pt, ascending=False)]\n",
    "    # Require exactly 2 leptons\n",
    "    mask_2lep = ak.num(sorted_leptons) == 2\n",
    "    # mask_loose_veto = ak.num(loose_leptons)\n",
    "    events_2lep = sorted_leptons[mask_2lep]\n",
    "    met_2lep = met_arrays[mask_2lep]\n",
    "    \n",
    "    if len(events_2lep) == 0:\n",
    "        return None, None, {}, None\n",
    "    # Get leading and subleading\n",
    "    leading = events_2lep[:, 0]\n",
    "    subleading = events_2lep[:, 1]\n",
    "    # Pre-Selection criteria\n",
    "    mask_1e1mu = ((leading.flavor == 11) & (subleading.flavor == 13)) | \\\n",
    "                 ((leading.flavor == 13) & (subleading.flavor == 11))\n",
    "    \n",
    "    # mask_opposite_charge = leading.charge * subleading.charge < 0\n",
    "\n",
    "    # CHARGE SELECTION LOGIC\n",
    "    if charge_req == 'OS':\n",
    "        mask_charge = leading.charge * subleading.charge < 0\n",
    "    elif charge_req == 'SS':\n",
    "        mask_charge = leading.charge * subleading.charge > 0\n",
    "    else:\n",
    "        raise ValueError(\"charge_req must be 'OS' or 'SS'\")\n",
    "    \n",
    "    mask_pt = (leading.pt > leading_pt_cut) & (subleading.pt > subleading_pt_cut)\n",
    "    \n",
    "    eta_leading = ((leading.flavor == 11) & (abs(leading.eta) < 2.5)) | \\\n",
    "                       ((leading.flavor == 13) & (abs(leading.eta) < 2.4))\n",
    "    \n",
    "    eta_subleading = ((subleading.flavor == 11) & (abs(subleading.eta) < 2.5)) | \\\n",
    "                          ((subleading.flavor == 13) & (abs(subleading.eta) < 2.4))\n",
    "    mask_eta = eta_leading & eta_subleading\n",
    "    \n",
    "    # Final selection\n",
    "    final_mask = mask_1e1mu & mask_opposite_charge & mask_pt & mask_eta\n",
    "    \n",
    "    # Store cutflow information\n",
    "    cutflow = {\n",
    "        'events_2lep': len(leading),\n",
    "        'events_1e1mu': ak.sum(mask_1e1mu),\n",
    "        'events_opposite_charge': ak.sum(mask_1e1mu & mask_opposite_charge),\n",
    "        'events_final': ak.sum(final_mask)\n",
    "    }\n",
    "    \n",
    "    return leading[final_mask], subleading[final_mask], cutflow, met_2lep[final_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb504f-07fc-4875-a248-dd53606eb20d",
   "metadata": {},
   "source": [
    "## Jet Selection and Lepton Cleaning\n",
    "\n",
    "Next up, we need to look at the jets in our event. Jets are essentially concentrated sprays of particles produced by quarks and gluons. \n",
    "\n",
    "First, we bundle the jet properties and apply some quality checks. For jets with lower momentum ($p_T < 50$ GeV), we also require a passing \"Pileup ID\".  \n",
    "\n",
    ">**Why?**\n",
    "> Because the LHC smashes many protons together at once, creating overlapping background collisions (pileup) that can fake low-energy jets. \n",
    "\n",
    "Then comes a crucial step: **Lepton Cleaning**. We calculate the angular distance, known as $\\Delta R$, between all of our jets and the tight leptons we selected earlier. If a jet is too close to an electron or muon ($\\Delta R < 0.4$), we throw the jet away.  \n",
    "\n",
    ">**Why do we do this?**\n",
    ">Because a high-energy electron deposits a lot of energy into the calorimeter, and the detector's clustering algorithms might accidentally label that energy splash as a \"jet\". We clean them to ensure we are not double-counting our leptons!\n",
    "\n",
    "Finally, we sort our surviving, clean jets by their $p_T$ and categorize the event into one of three bins: **0-jet**, **1-jet**, or **2-jet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939f6566-16f2-4cbf-b5af-37211a3d9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_jets(arrays, jet_pt_threshold=30, tight_leptons=None):\n",
    "    # Step 1: Create Jet object from individual arrays\n",
    "    jets = ak.zip({\n",
    "        \"pt\": arrays.Jet_pt,\n",
    "        \"eta\": arrays.Jet_eta,\n",
    "        \"phi\": arrays.Jet_phi,\n",
    "        \"mass\": arrays.Jet_mass,\n",
    "        \"jetId\": arrays.Jet_jetId,\n",
    "        \"btagDeepFlavB\": arrays.Jet_btagDeepFlavB,\n",
    "        \"puId\": arrays.Jet_puId \n",
    "    })\n",
    "    # Step 2: Good jet selection\n",
    "    pu_id_mask = (jets.pt > 50) | ((jets.pt <= 50) & (jets.puId >= 4))\n",
    "    good_mask = (jets.jetId >= 2) & (abs(jets.eta) < 4.7) & pu_id_mask\n",
    "    # Step 3: Lepton cleaning\n",
    "    if tight_leptons is not None and ak.max(ak.num(tight_leptons)) > 0:\n",
    "        # Delta R calculation\n",
    "        jets_eta = jets.eta[:, :, None]\n",
    "        jets_phi = jets.phi[:, :, None]\n",
    "        leps_eta = tight_leptons.eta[:, None, :]\n",
    "        leps_phi = tight_leptons.phi[:, None, :]\n",
    "        \n",
    "        deta = jets_eta - leps_eta\n",
    "        dphi = (jets_phi - leps_phi + np.pi) % (2 * np.pi) - np.pi\n",
    "        dr = np.sqrt(deta**2 + dphi**2)\n",
    "        \n",
    "        min_dr = ak.min(dr, axis=-1)\n",
    "        \n",
    "        min_dr = ak.fill_none(min_dr, 999.0)\n",
    "        good_mask = good_mask & (min_dr > 0.4)\n",
    "    \n",
    "    good_jets = jets[good_mask]\n",
    "    \n",
    "    # Step 4: Sort by pT\n",
    "    sorted_jets = good_jets[ak.argsort(good_jets.pt, axis=1, ascending=False)]\n",
    "    lead_jet_pt = ak.fill_none(ak.firsts(sorted_jets.pt), 0)\n",
    "    sublead_jet_pt = ak.fill_none(ak.firsts(sorted_jets[:, 1:].pt), 0)\n",
    "    # Step 5: Category masks based on jet count\n",
    "    isZeroJet = (lead_jet_pt < jet_pt_threshold)\n",
    "    isOneJet = (lead_jet_pt >= jet_pt_threshold) & (sublead_jet_pt < jet_pt_threshold)\n",
    "    isTwoJet = (sublead_jet_pt >= jet_pt_threshold) # At least 2 jets\n",
    "    \n",
    "    n_jets = ak.sum(sorted_jets.pt >= jet_pt_threshold, axis=1)\n",
    "    \n",
    "    return n_jets, good_mask, sorted_jets, isZeroJet, isOneJet, isTwoJet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008fe173-e130-4d40-a8cb-cd739d5d9972",
   "metadata": {},
   "source": [
    "## Hunting for b-jets (and why we veto them)\n",
    "\n",
    "Now we need to identify \"b-jets\"-jets that originate from bottom quarks.\n",
    "\n",
    "**Why do we care about b-jets?** Because of top quarks! A top quark decays almost exclusively into a W boson and a bottom quark. Since top-antitop pair production ($t\\bar{t}$) produces two W bosons, it mimics our $H \\to WW$ signal perfectly and is one of our biggest backgrounds. \n",
    "\n",
    "In the code below, we identify these b-jets and split our logic into two paths:\n",
    "* **The Signal Region Veto:** If we are looking for our Higgs signal, we *veto* (throw away) any event that contains a b-jet. This effectively kills that massive top quark background.\n",
    "* **The Top Control Region:** If we want to study our background to understand it better, we do the exact opposite. We deliberately *require* events to have b-jets, giving us a pure sample of top quarks to measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4498b99-4ec0-4d35-9542-031c66304cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bjet_categories(arrays, btag_threshold=0.2217, eta_max=2.5):\n",
    "    \"\"\"\n",
    "    Get different b-jet categories needed for SR/CR selection.\n",
    "    \"\"\"\n",
    "    # Base b-jet selection \n",
    "    base_bjet_mask = (\n",
    "        (arrays.Jet_jetId >= 2) &\n",
    "        (abs(arrays.Jet_eta) < eta_max) &\n",
    "        (arrays.Jet_btagDeepFlavB > btag_threshold)\n",
    "    )\n",
    "    # Different pT categories\n",
    "    bjets_20 = base_bjet_mask & (arrays.Jet_pt > 20)\n",
    "    bjets_20_30 = base_bjet_mask & (arrays.Jet_pt > 20) & (arrays.Jet_pt <= 30)\n",
    "    bjets_30 = base_bjet_mask & (arrays.Jet_pt > 30)\n",
    "    # Count per event\n",
    "    n_bjets_20 = ak.sum(bjets_20, axis=1)\n",
    "    n_bjets_20_30 = ak.sum(bjets_20_30, axis=1) \n",
    "    n_bjets_30 = ak.sum(bjets_30, axis=1)\n",
    "    return {\n",
    "        # For Signal Regions \n",
    "        'passes_bjet_veto': n_bjets_20 == 0,  \n",
    "        # For Control Regions\n",
    "        'has_btag_20_30': n_bjets_20_30 > 0,  # Top CR 0-jet\n",
    "        'has_btag_30': n_bjets_30 > 0,        # Top CR 1-jet, 2-jet\n",
    "        # Counts\n",
    "        'n_bjets_20': n_bjets_20,\n",
    "        'n_bjets_20_30': n_bjets_20_30,\n",
    "        'n_bjets_30': n_bjets_30\n",
    "    }\n",
    "\n",
    "def apply_bjet_selections(arrays):\n",
    "    bjet_info = get_bjet_categories(arrays)\n",
    "    # For Signal Regions\n",
    "    sr_bjet_veto = bjet_info['passes_bjet_veto']\n",
    "    # For Control Regions\n",
    "    cr_top_0jet = bjet_info['has_btag_20_30']\n",
    "    cr_top_1jet_2jet = bjet_info['has_btag_30']\n",
    "    return sr_bjet_veto, bjet_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbddda-62d1-4fcb-b059-61043f20f196",
   "metadata": {},
   "source": [
    "## Global Cuts\n",
    "\n",
    "With our particles clearly identified and cleaned, we now apply a set of \"global\" baseline cuts. Every single event must pass these filters, regardless of how many jets it has. \n",
    "\n",
    "Here are the three fundamental physics checks we perform in this block:\n",
    "\n",
    "* **Missing Transverse Energy (MET > 20 GeV):** We require a noticeable amount of missing energy in the transverse plane.\n",
    "\n",
    "  >**Why?**\n",
    "  >Because our $H \\to WW$ signal produces two neutrinos that escape the detector entirely! If an event has almost zero missing energy, it is highly unlikely to be the signal we are looking for.\n",
    "\n",
    "* **Dilepton Momentum ($p_T^{\\ell\\ell} > 30$ GeV):** The combined transverse momentum of our electron and muon system must be reasonably high. \n",
    "* **Invariant Mass ($m_{\\ell\\ell} > 12$ GeV):** The calculated invariant mass of the electron-muon pair must be at least 12 GeV. This low-mass threshold is a standard trick to strip away background noise from low-mass resonances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f41bae6-e1b7-4a76-8d5e-9076afe96a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_global_cuts(leading, subleading, met, mt_higgs, mt_l2_met,ptlls,masses):\n",
    "    \"\"\"Apply global selection cuts\"\"\"\n",
    "    # Global cuts\n",
    "    mask_met_pt = met.pt > 20\n",
    "    mask_ptll  = ptlls > 30\n",
    "    mask_mll = masses > 12 \n",
    "    # Combine all masks\n",
    "    global_mask =  mask_met_pt & mask_ptll & mask_mll\n",
    "    return global_mask, {\n",
    "        'pass_met_pt': ak.sum(mask_met_pt),\n",
    "        'pass_ptll': ak.sum(mask_ptll),\n",
    "        'pass_mll': ak.sum(mask_mll),\n",
    "        'pass_global': ak.sum(global_mask)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd627d53-c0ed-4c9f-ac94-304accb8d79f",
   "metadata": {},
   "source": [
    "## Defining the Signal Regions (SR)\n",
    "\n",
    "After all the cleaning and baseline filtering, we finally define our **Signal Regions**. A signal region is a highly restrictive subset of our data where we expect our $H \\to WW$ signal to be the most prominent compared to the background.\n",
    "\n",
    "To enter the signal region, an event must pass our previous global cuts, survive the b-jet veto, and pass two new, crucial kinematic thresholds:\n",
    "\n",
    "* **Higgs Transverse Mass ($m_T^H > 60$ GeV)**\n",
    "* **Subleading Lepton Transverse Mass ($m_T(\\ell_2, E_T^{miss}) > 30$ GeV)**\n",
    "\n",
    ">**Why do we use \"Transverse Mass\" instead of normal mass?** \\\n",
    ">Because of our invisible neutrinos! If we could detect all the decay products, we would simply calculate the standard invariant mass, and our signal would form a peak right at the Higgs mass (125 GeV). But since the neutrinos escape, we only have the *transverse* (perpendicular) components of the missing energy. \n",
    "\n",
    "By calculating the transverse mass ($m_T$), we can still reconstruct a recognizable shape for our Higgs signal, even with missing pieces! The cut on the subleading lepton's $m_T$ specifically helps us suppress backgrounds where a fake lepton or a mismeasured jet creates artificial missing energy.\n",
    "\n",
    "Finally, the function splits our surviving events into three distinct buckets:\n",
    "* **SR_0jet:** The cleanest region, dominated by standard gluon-gluon fusion.\n",
    "* **SR_1jet:** Slightly messier, but still holds a lot of signal.\n",
    "* **SR_2jet:** Here, we also apply an $m_{jj}$ (invariant mass of the two jets) window cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0fe5715-7b53-4543-9087-0451a5b2c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_signal_region_cuts(leading, subleading, met, masses, ptlls, mt_higgs, \n",
    "                           mt_l2_met, isZeroJet, isOneJet, isTwoJet, \n",
    "                           bjet_veto_mask, mjj=None):\n",
    "    \"\"\"Apply Signal Region selections for all jet categories.\"\"\"\n",
    "    sr_specific_cuts = (\n",
    "                        ( met.pt > 20) &\n",
    "                        ( ptlls > 30) &\n",
    "                       ( masses > 12)  &\n",
    "                        (mt_higgs >60) &\n",
    "                        (mt_l2_met >30) &\n",
    "                        bjet_veto_mask        \n",
    "                        )\n",
    "    sr_base =  sr_specific_cuts\n",
    "    # Apply mjj window for 2-jet category\n",
    "    if mjj is not None:\n",
    "        mjj_window = apply_mjj_window(mjj)\n",
    "    else:\n",
    "        mjj_window = ak.ones_like(isTwoJet, dtype=bool)\n",
    "    sr_regions = {\n",
    "        'SR_0jet': sr_base & isZeroJet,\n",
    "        'SR_1jet': sr_base & isOneJet,\n",
    "        'SR_2jet': sr_base & isTwoJet & mjj_window\n",
    "    }\n",
    "    return sr_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce22ac-de4b-4a09-8812-6f129f8e93ac",
   "metadata": {},
   "source": [
    "## Defining the Control Regions (CR)\n",
    "\n",
    "While the Signal Region is designed to capture our Higgs boson, a **Control Region** does the exact opposite. A control region is a specialized subset of data deliberately designed to be entirely background! \n",
    "\n",
    ">**Why do we want a region full of background?**\\\n",
    ">Because simulations aren't perfect. By creating a region rich in a specific background (like top quarks or Z bosons) but free of any Higgs signal, we can compare our Monte Carlo simulations to real Data. If they match well in the Control Region, we can trust our background estimates in the Signal Region. \n",
    "\n",
    "In this block, we define two main types of control regions:\n",
    "\n",
    "### 1. The Top Control Region (`CR_top`)\n",
    "To isolate events where top quarks were produced, we take our baseline cuts and explicitly *require* the presence of b-tagged jets. \n",
    "\n",
    "We also require the invariant mass of the leptons to be greater than 50 GeV ($m_{\\ell\\ell} > 50$). Since, top quarks are massive, and the leptons they produce tend to have a wide opening angle and high momentum, pushing their combined invariant mass higher.\n",
    "\n",
    "### 2. The Tau-Tau Control Region (`CR_tau`)\n",
    "One of our trickiest backgrounds is the Drell-Yan process where a Z boson decays into two tau leptons ($Z \\to \\tau\\tau$), which then decay into an electron, a muon, and *four* neutrinos! \n",
    "\n",
    "To isolate this background, we do two things:\n",
    "* **We invert the Higgs cut:** We require $m_T^H < 60$ GeV. Since our Signal Region required this to be *greater* than 60, this ensures our Tau-Tau Control Region has absolutely zero overlap with our signal.\n",
    "* **We apply a shifted Z-mass window:** We restrict the lepton invariant mass to be between 40 and 80 GeV.\n",
    "  >**Why not the actual Z boson mass of 91 GeV?** \\\n",
    "  > Because the tau leptons decay and release neutrinos, carrying away a chunk of the energy. This shifts the visible mass of the electron-muon pair significantly lower than the true Z mass peak!\n",
    "\n",
    "Just like before, we split these control regions into 0-jet, 1-jet, and 2-jet categories to match the structure of our signal regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b95033-2c68-4c4c-8091-ec3da603f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_control_region_cuts(leading, subleading, met, masses, ptlls, mt_higgs, \n",
    "                            mt_l2_met, isZeroJet, isOneJet, isTwoJet, \n",
    "                            bjet_info, mjj=None):\n",
    "    \"\"\"Apply Control Region selections for all jet categories.\"\"\"\n",
    "    cr_base = (\n",
    "        (met.pt > 20) &\n",
    "        (ptlls > 30) &\n",
    "        (mt_l2_met > 30)    &\n",
    "        (masses > 12)\n",
    "    )\n",
    "    # Apply mjj window for 2-jet category\n",
    "    if mjj is not None:\n",
    "        mjj_window = apply_mjj_window(mjj)\n",
    "    else:\n",
    "        mjj_window = ak.ones_like(isTwoJet, dtype=bool)\n",
    "    cr_regions = {}\n",
    "    # === TOP CONTROL REGIONS ===\n",
    "    cr_top_base = cr_base & (masses > 50)\n",
    "    cr_regions['CR_top_0jet'] = (\n",
    "        cr_top_base & \n",
    "        isZeroJet & \n",
    "        bjet_info['has_btag_20_30']  # 20 < pT < 30 GeV b-jets for 0-jet\n",
    "    )\n",
    "    cr_regions['CR_top_1jet'] = (\n",
    "        cr_top_base & \n",
    "        isOneJet & \n",
    "        bjet_info['has_btag_30']     # pT > 30 GeV b-jets for 1-jet\n",
    "    )\n",
    "    cr_regions['CR_top_2jet'] = (\n",
    "        cr_top_base & \n",
    "        isTwoJet & \n",
    "        mjj_window &\n",
    "        bjet_info['has_btag_30']     # pT > 30 GeV b-jets for 2-jet\n",
    "    )\n",
    "    # === TAU-TAU CONTROL REGIONS ===\n",
    "    cr_tau_base = (\n",
    "        cr_base & \n",
    "        (mt_higgs < 60) & \n",
    "        (masses > 40) & \n",
    "        (masses < 80) &\n",
    "        bjet_info['passes_bjet_veto']  \n",
    "    )\n",
    "    cr_regions['CR_tau_0jet'] = cr_tau_base & isZeroJet\n",
    "    cr_regions['CR_tau_1jet'] = cr_tau_base & isOneJet  \n",
    "    cr_regions['CR_tau_2jet'] = cr_tau_base & isTwoJet & mjj_window\n",
    "\n",
    "    # === WW CONTROL REGIONS (NEW) ===\n",
    "    # Require b-jet veto (to kill Top) and masses > 100 (to kill Higgs)\n",
    "    cr_ww_base = (\n",
    "        cr_base & \n",
    "        bjet_info['passes_bjet_veto'] & \n",
    "        (masses > 100)  \n",
    "    )\n",
    "    cr_regions['CR_WW_0jet'] = cr_ww_base & isZeroJet\n",
    "    cr_regions['CR_WW_1jet'] = cr_ww_base & isOneJet\n",
    "    cr_regions['CR_WW_2jet'] = cr_ww_base & isTwoJet & mjj_window\n",
    "    return cr_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c283c6c-9c9f-43e3-8298-edb25860225f",
   "metadata": {},
   "source": [
    "## Calculating Kinematic Variables\n",
    "\n",
    "Now that we have isolated the exact particles we want, we need to calculate the final variables that will define our signal regions and fill our histograms. \n",
    "\n",
    "To do this, we use the `cal_kinematic_var` function. You will notice the very first thing we do is pass our raw particle data into the `vector` library to create a `lepton_vector`. \n",
    "\n",
    ">**Why do we use a special `vector` library for this?**\n",
    ">Because these particles are moving at nearly the speed of light in three-dimensional space! Calculating how their energies and momenta combine using standard trigonometry is a massive headache. The `vector` library handles all that complex relativistic math behind the scenes. \n",
    "\n",
    "Instead of writing out long equations, we can treat the particles as simple objects. If we want to know the properties of the combined electron-muon system, we literally just add them together in the code: `dilepton = lepton_1 + lepton_2`. The library instantly calculates their combined invariant mass and momentum for us!\n",
    "\n",
    "Once we have our vectors set up, we calculate a few key features:\n",
    "\n",
    "* **Mass and Momentum (`masses`, `ptll`):** We simply ask our new `dilepton` object for its combined mass and transverse momentum.\n",
    "* **Angular Difference (`dphi`):** We calculate the angle between the electron and the muon. \n",
    "* **Transverse Mass (`mt_higgs`, `mt_l2_met`):** Finally, we mix the properties of our visible leptons with the Missing Transverse Energy (MET). As we discussed earlier, this helps us reconstruct the mass of the Higgs and the W boson even though the neutrinos escaped the detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95404347-2651-40ee-95bb-a9ebeec9b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_angle_to_pi(angle):\n",
    "    return (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "def create_lepton_vector(lepton):\n",
    "    \"\"\"Create 4-vector from lepton properties \"\"\"\n",
    "    return vector.array({\n",
    "        \"pt\": lepton.pt,\n",
    "        \"eta\": lepton.eta,\n",
    "        \"phi\": lepton.phi,\n",
    "        \"mass\": lepton.mass\n",
    "    })\n",
    "    \n",
    "def cal_kinematic_var(leading, subleading, met):\n",
    "    # Create vectors\n",
    "    lepton_1 = create_lepton_vector(leading)\n",
    "    lepton_2 = create_lepton_vector(subleading)\n",
    "    dilepton = lepton_1 + lepton_2\n",
    "    #  Basic Variables\n",
    "    masses = dilepton.mass\n",
    "    ptll = dilepton.pt\n",
    "    dphi = wrap_angle_to_pi(leading.phi - subleading.phi)\n",
    "    # Higgs Transverse Mass \n",
    "    dll_et = np.sqrt(dilepton.pt**2 + dilepton.mass**2)\n",
    "    mt_higgs_dphi = wrap_angle_to_pi(dilepton.phi - met.phi)\n",
    "    term_1 = masses**2\n",
    "    term_2 = 2 * (dll_et * met.pt - dilepton.pt * met.pt * np.cos(mt_higgs_dphi))\n",
    "    mt_higgs = np.sqrt(term_1 + term_2)\n",
    "    # Lepton 2 Transverse Mass\n",
    "    mt_l2_met_dphi = wrap_angle_to_pi(subleading.phi - met.phi)\n",
    "    mt_l2_met = np.sqrt(2 * subleading.pt * met.pt * (1 - np.cos(mt_l2_met_dphi)))\n",
    "\n",
    "    return masses, ptll, dphi, mt_higgs, mt_l2_met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de62e7c-a5ca-4132-8a32-6a260311616a",
   "metadata": {},
   "source": [
    "## The 2-Jet Category and the Mass Window\n",
    "\n",
    "Remember earlier when we sorted our events into 0-jet, 1-jet, and 2-jet categories? If an event lands in the 2-jet bucket, we need to take one extra step to analyze the relationship between those two jets.\n",
    "\n",
    "First, we use `calculate_mjj`. Just like we did with our electron and muon, we use 4-vectors to combine the two highest-energy jets and calculate their combined invariant mass ($m_{jj}$). \n",
    "\n",
    "Next, we pass that mass into `apply_mjj_window`. If you look closely at the math, you will notice we keep events where $m_{jj} < 65$ or $m_{jj} > 105$. This means we are deliberately *excluding* all events that land right in the middle between 65 and 105 GeV. \n",
    "\n",
    "> **Why do we cut out the middle?**\\\n",
    "> Because that specific mass range is exactly where the W and Z bosons live! \n",
    "\n",
    "If the two jets have a combined mass in that 65-105 GeV window, they most likely came from a background W or Z boson decaying into quarks, rather than the specific Higgs process we are hunting for. Slicing out that middle part is a effective way to cut down our background!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc57db2f-9acf-4d03-944d-731e4a630b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mjj(jets):\n",
    "    # Get number of jets per event\n",
    "    n_jets = ak.num(jets)\n",
    "    # Initialize mjj with zeros for all events\n",
    "    mjj = ak.zeros_like(n_jets, dtype=float)\n",
    "    # Create mask for events with at least 2 jets\n",
    "    has_two_jets = n_jets >= 2\n",
    "    # Only proceed if there are events with 2+ jets\n",
    "    if ak.any(has_two_jets):\n",
    "        # Use padding to safely access indices\n",
    "        jets_padded = ak.pad_none(jets, 2, axis=1)\n",
    "        # Create 4-vectors for jets\n",
    "        jet_vectors = ak.zip({\n",
    "            \"pt\": ak.fill_none(jets_padded.pt, 0.0),\n",
    "            \"eta\": ak.fill_none(jets_padded.eta, 0.0),\n",
    "            \"phi\": ak.fill_none(jets_padded.phi, 0.0),\n",
    "            \"mass\": ak.fill_none(jets_padded.mass, 0.0)\n",
    "        }, with_name=\"Momentum4D\")\n",
    "        # Get first two jets\n",
    "        jet1 = jet_vectors[:, 0]\n",
    "        jet2 = jet_vectors[:, 1]\n",
    "        # Calculate invariant mass using vector addition\n",
    "        dijet = jet1 + jet2\n",
    "        mjj_calculated = dijet.mass\n",
    "        # Apply only where we have 2+ jets\n",
    "        mjj = ak.where(has_two_jets, mjj_calculated, 0.0)\n",
    "    \n",
    "    return mjj\n",
    "\n",
    "def apply_mjj_window(mjj):\n",
    "    return (mjj < 65) | ((mjj > 105) & (mjj < 120))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3cf1ec-d611-4059-a3f0-505ca589fb41",
   "metadata": {},
   "source": [
    "# **DATA-MC Corrections**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25bc39-a844-4586-868c-209ab6764ed0",
   "metadata": {},
   "source": [
    "## Scaling the Monte Carlo Simulations\n",
    "\n",
    "Next, we define our `LUMINOSITY` and a large dictionary containing the specific properties of our Monte Carlo (MC) samples.\n",
    "\n",
    "> **Why do we need to scale our simulations?**  \n",
    "> Because MC generators create as many events as we ask them to! A simulation might generate 10 million top quark events and 10 million Higgs events so that we have enough statistics to study them. However, in the real detector, top quarks are produced thousands of times more often than Higgs bosons.\n",
    "\n",
    "To make our simulated backgrounds accurately represent the real world, we must scale each MC event so that the proportions match reality. We do this using three key numbers:\n",
    "\n",
    "* **Luminosity ($L$):** The total amount of data collected. For our [2016G and 2016H data](https://opendata.cern.ch/record/1059), this is $16{,}393 \\text{ pb}^{-1}$. You can read [this](https://cds.cern.ch/record/941318/files/p361.pdf) if you want to learn more about luminosity in collisions.\n",
    "* **Cross Section ($\\sigma$):** The fundamental quantum mechanical probability of a specific physics process occurring during a protonâproton collision.\n",
    "* **Sum of Generator Weights:** The total initial weight of all the simulated events in that specific dataset. `genWeight` is a branch inside our tree that we extracted from the MC ROOT files earlier.\n",
    "\n",
    "Later in the processor, we will combine these to calculate a universal scale factor for every simulated event:\n",
    "\n",
    "$$\n",
    "\\text{Scale Factor} = \\frac{\\sigma \\times L \\times \\text{genWeight}}{\\sum \\text{genWeight}}\n",
    "$$\n",
    "\n",
    "The `get_sample_key` function below acts as our lookup tool. When we feed a file into our processor, it checks the filename against this dictionary to retrieve the correct cross section. Notice how it deliberately skips filenames containing `Run2016` or `SingleMuon`? That is because real collision data is already physicalâit should not be scaled.\n",
    "\n",
    "*Tip: If you want to dive deeper into exactly where these samples and cross-section numbers come from, refer to the **`Datasets/README_MC_Samples_2016UL.md`** and to see how the Sum_genweight is calculated refer **`notebooks/sum_genWeight`** file for detailed dataset documentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b4366a0-f830-4cd4-b581-d33ce62d2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luminosity for 2016 UltraLegacy  (pb^-1)\n",
    "LUMINOSITY = 16_393.0 \n",
    "sample_info_detailed = {\n",
    "    #   DRELL-YAN  \n",
    "    \"DYJetsToLL_M-50\":      { \"xsec\": 6025.20, \"sum_genWeight\": 82448537.0 },\n",
    "    #   TOP QUARK  \n",
    "    \"TTTo2L2Nu\":            { \"xsec\": 87.31, \"sum_genWeight\": 3140127171.4748 },\n",
    "    \"ST_t-channel_top\":     { \"xsec\": 44.33, \"sum_genWeight\": 6703802049.126 },\n",
    "    \"ST_t-channel_antitop\": { \"xsec\": 26.38, \"sum_genWeight\": 1522100315.652 },\n",
    "    \"ST_tW_top\":            { \"xsec\": 35.60, \"sum_genWeight\": 20635251.1008 },\n",
    "    \"ST_tW_antitop\":        { \"xsec\": 35.60, \"sum_genWeight\": 27306324.658 },\n",
    "    \"ST_s-channel\":         { \"xsec\": 3.36,  \"sum_genWeight\": 19429336.179 },\n",
    "    #   FAKES  \n",
    "    \"WJetsToLNu\":           { \"xsec\": 61526.7, \"sum_genWeight\": 9697410121705.164 },\n",
    "    \"TTToSemiLeptonic\":     { \"xsec\": 364.35,  \"sum_genWeight\": 43548253725.284 },\n",
    "    #   V+GAMMA  \n",
    "    \"ZGToLLG\":              { \"xsec\": 131.30,   \"sum_genWeight\": 3106465270.711 },\n",
    "    \"WGToLNuG\":             { \"xsec\": 405.271, \"sum_genWeight\": 3353413.0 },\n",
    "    #   DIBOSON  \n",
    "    \"WZTo3LNu\":             { \"xsec\": 4.42965, \"sum_genWeight\": 4077550.6318 },\n",
    "    \"WZTo2Q2L\":             { \"xsec\": 5.595,   \"sum_genWeight\": 129756627.882 },\n",
    "    \"ZZ\":                   { \"xsec\": 16.523,  \"sum_genWeight\": 1151000.0 },\n",
    "    #   SIGNAL & IRREDUCIBLE  \n",
    "    \"GluGluToWW\":           { \"xsec\": 0.5905, \"sum_genWeight\": 17662000.0 },\n",
    "    \"WWTo2L2Nu\":            { \"xsec\": 12.178,  \"sum_genWeight\": 32147079.595 },\n",
    "    \"Higgs\":                { \"xsec\": 1.0315,  \"sum_genWeight\": 63281816.82 }\n",
    "}\n",
    "\n",
    "\n",
    "def get_sample_key(filename, dict_for_xsec_mapping):\n",
    "    fn = filename\n",
    "    if any(x in fn for x in [\"Run2016\", \"SingleMuon\", \"DoubleEG\", \"MuonEG\"]):\n",
    "        return None\n",
    "    for key, value in dict_for_xsec_mapping.items():\n",
    "        if key in fn:\n",
    "            return value\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f8f89-2fa5-4ec4-9c69-747d9203d024",
   "metadata": {},
   "source": [
    "## Applying Scale Factors to the Simulation\n",
    "\n",
    "In this step, we apply two important corrections to ensure that our Monte Carlo simulation reflects the real CMS detector as closely as possible.\n",
    "\n",
    "### 1. Lepton ID and Isolation Scale Factors\n",
    "\n",
    "Even with a very detailed detector simulation, the efficiency for identifying and isolating leptons in Monte Carlo is not perfectly identical to what is observed in real data. Small differences arise from detector response, reconstruction algorithms, and calibration effects.\n",
    "\n",
    "To correct for this, we load precomputed scale factor (SF) lookup tables from `Auxillary_files/lepID_lookup.txt`. These tables encode data-to-simulation efficiency ratios as functions of lepton transverse momentum ($p_T$) and pseudorapidity ($\\eta$).\n",
    "\n",
    "Later, inside the event loop, the `get_sf_with_uncertainty` function evaluates each simulated leptonâs $p_T$ and $\\eta$, retrieves the appropriate correction factor from the lookup table, and assigns a weight to that particle. In effect, every simulated lepton is adjusted so that the overall identification and isolation efficiency matches what was measured in real collision data.\n",
    "To see how the look-up table has been created and how these numbers are obtained see **`notebooks/Muon_EFF.ipynb`** and **`notebooks/Eff_txt_file_cleaning.ipynb`** \\\n",
    "\n",
    "\n",
    "**NOTE TO SELF: ADD THE LOCATIONS OF THIS NOTEBOOKS**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. High-Level Trigger (HLT) Efficiency\n",
    "\n",
    "Unline real collision data, the MC samples do not pass any trigger. To account for this difference, we apply a global trigger scale factor.\n",
    "A detailed procedure for getting the Trigger SF is shown in **`notebooks/Trigger_efficiency.ipynb`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62803614-18af-48ad-8531-17183c02b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lepID_data(filepath):\n",
    "    \"\"\"Reads a text file containing Python variables and returns a dictionary of those variables.\"\"\"\n",
    "    local_vars = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        exec(f.read(), {}, local_vars)\n",
    "        \n",
    "    return local_vars\n",
    "    \n",
    "def get_sf_with_uncertainty(eta_array, pt_array, lookup_table):\n",
    "    sf_out = ak.ones_like(eta_array, dtype=float)\n",
    "    err_out = ak.zeros_like(eta_array, dtype=float)\n",
    "    eta_abs = abs(eta_array)\n",
    "    for (eta_min, eta_max, pt_min, pt_max, sf_val, err_val) in lookup_table:\n",
    "        mask = (eta_abs >= eta_min) & (eta_abs < eta_max) & \\\n",
    "               (pt_array >= pt_min) & (pt_array < pt_max)\n",
    "        sf_out = ak.where(mask, sf_val, sf_out)\n",
    "        err_out = ak.where(mask, err_val, err_out)\n",
    "    return sf_out, err_out\n",
    "    \n",
    "lep_data = load_lepID_data(LEPTON_ID_SF)\n",
    "\n",
    "ELECTRON_SF_DATA = lep_data.get('ELECTRON_SF_DATA', [])\n",
    "MUON_ISO_DATA = lep_data.get('MUON_ISO_DATA', [])\n",
    "MUON_TIGHT_DATA = lep_data.get('MUON_TIGHT_DATA', [])\n",
    "\n",
    "TRIGGER_SF_VAL = 0.9129\n",
    "TRIGGER_SF_ERR = 0.0008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7e7fc-fbf3-4702-8c6e-b42b9b383751",
   "metadata": {},
   "source": [
    "# Histogram Initialization\n",
    "\n",
    "With the event selection done, we need a structured way to store the results. In this block, we initialize the empty histograms that will eventually hold all of our processed data.\n",
    "\n",
    "We start by defining the **variations** for our systematic uncertainties.\n",
    "\n",
    "Next, we define `variables_to_plots`. This dictionary uses the `hist` library to set up the axes, bin counts, and ranges for all the kinematic variables we want to study (like $m_{\\ell\\ell}$, MET, and $\\Delta\\phi$).\n",
    "\n",
    "Finally, the `initialize_stage_histograms` function builds a nested dictionary to keep everything highly organized. For every physics sample (like Data, Top quarks, or our Higgs signal), it generates a dedicated histogram for:\n",
    "1. Every stage/region of our analysis (e.g., SR_0jet, CR_top_1jet)\n",
    "2. Every kinematic variable\n",
    "3. Every systematic variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18810c6f-0962-4349-bdb8-33588d73954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram storage initialized for 8 samples.\n"
     ]
    }
   ],
   "source": [
    "VARIATIONS = ['nominal', 'trigger_up', 'trigger_down', 'ele_id_up', 'ele_id_down', 'mu_id_up', 'mu_id_down']\n",
    "\n",
    "variables_to_plots = {\n",
    "    'mass': hist.axis.Regular(20, 0, 200, name=\"mass\", label=\"m_ll [GeV]\"),\n",
    "    'met': hist.axis.Regular(20, 0, 200, name=\"met\", label=\"MET [GeV]\"),\n",
    "    'dphi': hist.axis.Regular(20, 0, np.pi, name=\"dphi\", label=\"dphi(l,l)\"),\n",
    "    'ptll': hist.axis.Regular(20, 0, 200, name=\"ptll\", label=\"p_T^ll [GeV]\"),\n",
    "    'mt_higgs': hist.axis.Regular(20, 0, 300, name=\"mt_higgs\", label=\"m_T^H [GeV]\"),\n",
    "    'mt_l2_met': hist.axis.Regular(20, 0, 200, name=\"mt_l2_met\", label=\"m_T(l2,MET) [GeV]\"),\n",
    "    'mjj': hist.axis.Regular(20, 0, 500, name=\"mjj\", label=\"m_jj [GeV]\"),\n",
    "    'leading_pt': hist.axis.Regular(20, 0, 200, name=\"leading_pt\", label=\"Leading lepton p_T [GeV]\"),\n",
    "    'subleading_pt': hist.axis.Regular(20, 0, 200, name=\"subleading_pt\", label=\"Subleading lepton p_T [GeV]\"),\n",
    "}\n",
    "\n",
    "def initialize_stage_histograms(stages_list, vars_dict, variations_list):\n",
    "    stage_histograms = {}\n",
    "    for stage in stages_list:\n",
    "        stage_histograms[stage] = {}\n",
    "        for var_name, axis in vars_dict.items():\n",
    "            stage_histograms[stage][var_name] = {}\n",
    "            for syst in variations_list:\n",
    "                stage_histograms[stage][var_name][syst] = hist.Hist(axis, storage=hist.storage.Weight())\n",
    "    return stage_histograms\n",
    "\n",
    "hist_data = {}\n",
    "\n",
    "\n",
    "for label in sorted(stack_order.keys(), key=lambda x: stack_order[x]):\n",
    "    hist_data[label] = initialize_stage_histograms(stage_names, variables_to_plots, VARIATIONS)\n",
    "\n",
    "print(f\"Histogram storage initialized for {len(hist_data)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd4ac29-6dd4-4809-878e-a149d3c3fa6e",
   "metadata": {},
   "source": [
    "# **Run Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38de65-8ae2-475c-b114-918c68357de3",
   "metadata": {},
   "source": [
    "## The Processor Function\n",
    "\n",
    "We have finally arrived at the engine of our analysis: the `make_processor` function. This block ties together every single helper function, cut, and scale factor we have defined so far into one massive, automated assembly line.\n",
    "\n",
    "Here is a flowchart mapping out exactly how an event travels through our logic, from raw data to a filled histogram:\n",
    "\n",
    "![All the steps involved](../Images/flowchart.svg)\n",
    "\n",
    "*Note: The flowchart illustrates our entire workflow, but the code in this block stops at the histogram-saving stage. The remaining steps are covered below.*\n",
    "\n",
    "\n",
    "While the flowchart covers the step-by-step physics, there are a few clever programming tricks happening in this block that are worth highlighting:\n",
    "\n",
    "### 1. The Distributed Worker Setup\n",
    "You might notice that we put `import uproot`, `import awkward`, and other libraries *inside* the `processing_file` function. \\\n",
    ">**Why import them again inside the function?**\\\n",
    ">Because we are designing this code to run on a distributed cluster using Dask. If we put the imports at the top of our notebook, the remote worker wouldn't have access to them. By putting them inside the worker function, we guarantee that every single node loads its own tools before it starts working.\n",
    "\n",
    "### 2. The Cutflow Tracker\n",
    "Throughout the code, you will see us adding numbers to `cutflow` and `weighted_cutflow` dictionaries. This acts as our accounting book. Every time we apply a cut (like filtering for 2 leptons, or vetoing b-jets), we record exactly how many events survived.\n",
    "\n",
    "### 3. The `fill_histograms` Helper\n",
    "Filling 9 different histograms across 7 systematic variations (like `trigger_up` or `mu_id_down`) for multiple regions would normally require hundreds of lines of repetitive code. Instead, we wrote a compact helper function that automatically applies the correct event mask and the correct systematic weight to all variables at once. \n",
    "\n",
    "### 4. Network Resilience\n",
    "At the bottom of the loop, there is a `try/except` block that will attempt to read a file up to 3 times before giving up. \n",
    ">**Why do we need retries?** \\\n",
    ">Because we are streaming ROOT files over the internet from distant servers. Small network issues and dropped connections are inevitable! Instead of letting a 2-second network glitch crash our entire hours-long analysis, we tell the worker to sleep for 3 seconds and simply try again. If it completely fails, it returns empty histograms so the rest of the analysis can continue uninterrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d8c74de-d223-429b-bfe4-5469cd95c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processor(golden_json_data, sample_info_detailed, luminosity, run_periods):\n",
    "    #        WORKER FUNCTION      \n",
    "    def processing_file(label, file_url, file_idx):\n",
    "        import uproot\n",
    "        import awkward as ak\n",
    "        import numpy as np\n",
    "        import vector\n",
    "        import time\n",
    "        import hist \n",
    "        \n",
    "        vector.register_awkward()\n",
    "        \n",
    "        file_name = file_url.split('/')[-1] \n",
    "        is_data = (label == 'Data')\n",
    "        \n",
    "        specific_sample_key = get_sample_key(file_url,dict_for_xsec_mapping)\n",
    "\n",
    "        empty_cutflow = {stage: 0 for stage in cutflow_stages}\n",
    "        \n",
    "        #        Updated Fill Function      \n",
    "        def fill_histograms(stage_name, mask, weights_dict, \n",
    "                           masses, met_pt, dphis, ptlls,\n",
    "                           mt_higgs, mt_l2_met, mjj,\n",
    "                           leading_pt, subleading_pt):\n",
    "            \n",
    "            if not isinstance(mask, np.ndarray):\n",
    "                mask = ak.to_numpy(mask)\n",
    "\n",
    "            if np.sum(mask) == 0:\n",
    "                return\n",
    "\n",
    "            def masked(arr, flatten=False):\n",
    "                sliced = arr[mask]\n",
    "                if flatten:\n",
    "                    sliced = ak.flatten(sliced)\n",
    "                return ak.to_numpy(sliced)\n",
    "\n",
    "            # Loop over all systematic variations\n",
    "            for syst in VARIATIONS:\n",
    "                w_syst = weights_dict.get(syst, weights_dict['nominal'])\n",
    "                w = masked(w_syst)\n",
    "\n",
    "                stage_histograms[stage_name]['mass'][syst].fill(masked(masses), weight=w)\n",
    "                stage_histograms[stage_name]['met'][syst].fill(masked(met_pt), weight=w)\n",
    "                stage_histograms[stage_name]['dphi'][syst].fill(masked(dphis), weight=w)\n",
    "                stage_histograms[stage_name]['ptll'][syst].fill(masked(ptlls), weight=w)\n",
    "                stage_histograms[stage_name]['mt_higgs'][syst].fill(masked(mt_higgs), weight=w)\n",
    "                stage_histograms[stage_name]['mt_l2_met'][syst].fill(masked(mt_l2_met), weight=w)\n",
    "                stage_histograms[stage_name]['mjj'][syst].fill(masked(mjj), weight=w)\n",
    "                stage_histograms[stage_name]['leading_pt'][syst].fill(masked(leading_pt), weight=w)\n",
    "                stage_histograms[stage_name]['subleading_pt'][syst].fill(masked(subleading_pt), weight=w)\n",
    "        \n",
    "        try:\n",
    "            # Use GLOBAL initialization\n",
    "            stage_histograms = initialize_stage_histograms(stage_names, variables_to_plots, VARIATIONS)\n",
    "            \n",
    "            cutflow = empty_cutflow.copy()\n",
    "            weighted_cutflow = {stage: 0.0 for stage in cutflow_stages}\n",
    "            \n",
    "            max_file_retries = 3\n",
    "            \n",
    "            for file_attempt in range(max_file_retries):\n",
    "                try:\n",
    "                    for arrays in load_events(file_url, batch_size= 1_000_000, is_data=is_data):\n",
    "                        \n",
    "                        cutflow['total'] += len(arrays)\n",
    "\n",
    "                        #        SCALING & VARIATIONS INITIALIZATION      \n",
    "                        if is_data:\n",
    "                            base_weight = ak.ones_like(arrays.PuppiMET_pt, dtype=float)\n",
    "                        elif specific_sample_key in sample_info_detailed:\n",
    "                            info = sample_info_detailed[specific_sample_key]\n",
    "                            scale_factor = (info['xsec'] * luminosity) / info['sum_genWeight']\n",
    "                            base_weight = arrays.genWeight * scale_factor\n",
    "                        else:\n",
    "                            base_weight = ak.zeros_like(arrays.PuppiMET_pt, dtype=float)\n",
    "\n",
    "                        # Initialize Dictionary of Weights\n",
    "                        base_weights_dict = {v: base_weight for v in VARIATIONS}\n",
    "\n",
    "                        #        APPLY TRIGGER SF & UNCERTAINTY (MC ONLY)      \n",
    "                        if not is_data:\n",
    "                            base_weights_dict['nominal'] = base_weights_dict['nominal'] * TRIGGER_SF_VAL\n",
    "                            base_weights_dict['trigger_up']   = base_weights_dict['trigger_up'] * (TRIGGER_SF_VAL + TRIGGER_SF_ERR)\n",
    "                            base_weights_dict['trigger_down'] = base_weights_dict['trigger_down'] * (TRIGGER_SF_VAL - TRIGGER_SF_ERR)\n",
    "                            \n",
    "                            for var in ['ele_id_up', 'ele_id_down', 'mu_id_up', 'mu_id_down']:\n",
    "                                base_weights_dict[var] = base_weights_dict[var] * TRIGGER_SF_VAL\n",
    "\n",
    "                        weighted_cutflow['total'] += float(ak.sum(base_weights_dict['nominal']))\n",
    "                        \n",
    "                        #        JSON MASK (DATA ONLY)      \n",
    "                        if is_data and golden_json_data is not None:\n",
    "                            try:\n",
    "                                json_mask = apply_json_mask(arrays, golden_json_data, run_periods=run_periods)\n",
    "                                n_events_after = int(ak.sum(json_mask))\n",
    "                                cutflow['after_json'] += n_events_after\n",
    "                                weighted_cutflow['after_json'] += float(ak.sum(base_weights_dict['nominal'][json_mask]))\n",
    "\n",
    "                                if n_events_after == 0:\n",
    "                                    continue\n",
    "\n",
    "                                arrays = arrays[json_mask]\n",
    "                                for k in base_weights_dict:\n",
    "                                    base_weights_dict[k] = base_weights_dict[k][json_mask]\n",
    "                            except Exception as e: \n",
    "                                print(f\"Warning: JSON mask failed for {file_name}: {e}\")\n",
    "                        \n",
    "                        #        LEPTON PREPARATION      \n",
    "                        tight_leptons, _, _ = select_tight_leptons(arrays)\n",
    "                        met = ak.zip({\"pt\": arrays.PuppiMET_pt, \"phi\": arrays.PuppiMET_phi})\n",
    "                        \n",
    "                        sorted_leptons = tight_leptons[ak.argsort(tight_leptons.pt, ascending=False)]\n",
    "                        has_2lep = ak.num(sorted_leptons) == 2\n",
    "                        events_2lep = sorted_leptons[has_2lep]\n",
    "                        \n",
    "                        if len(events_2lep) == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        lead_all = events_2lep[:, 0]\n",
    "                        sublead_all = events_2lep[:, 1]\n",
    "                        \n",
    "                        # Base kinematic masks\n",
    "                        mask_1e1mu = ((lead_all.flavor == 11) & (sublead_all.flavor == 13)) | \\\n",
    "                                     ((lead_all.flavor == 13) & (sublead_all.flavor == 11))\n",
    "                        mask_pt = (lead_all.pt > 25) & (sublead_all.pt > 13)\n",
    "                        \n",
    "                        eta_leading = ((lead_all.flavor == 11) & (abs(lead_all.eta) < 2.5)) | \\\n",
    "                                      ((lead_all.flavor == 13) & (abs(lead_all.eta) < 2.4))\n",
    "                        eta_subleading = ((sublead_all.flavor == 11) & (abs(sublead_all.eta) < 2.5)) | \\\n",
    "                                         ((sublead_all.flavor == 13) & (abs(sublead_all.eta) < 2.4))\n",
    "                        mask_eta = eta_leading & eta_subleading\n",
    "                        \n",
    "                        base_emu_mask = mask_1e1mu & mask_pt & mask_eta\n",
    "                        indices_2lep = ak.where(has_2lep)[0]\n",
    "\n",
    "                        # PRE-CALCULATE JETS (Run once per chunk for efficiency)\n",
    "                        n_jets_full, _, sorted_jets_full, isZeroJet_full, isOneJet_full, isTwoJet_full = count_jets(\n",
    "                            arrays, tight_leptons=tight_leptons\n",
    "                        )\n",
    "                        mjj_full = calculate_mjj(sorted_jets_full)\n",
    "                        bjet_veto_full, bjet_info_full = apply_bjet_selections(arrays)\n",
    "\n",
    "                        #        CHARGE LOOP (OS and SS)      \n",
    "                        charge_configs = [\n",
    "                            ('OS', lead_all.charge * sublead_all.charge < 0),\n",
    "                            ('SS', lead_all.charge * sublead_all.charge > 0)\n",
    "                        ]\n",
    "\n",
    "                        for charge_req, charge_mask in charge_configs:\n",
    "                            emu_mask_2lep = base_emu_mask & charge_mask\n",
    "                            indices_selected = ak.to_numpy(indices_2lep[emu_mask_2lep])\n",
    "                            \n",
    "                            if len(indices_selected) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            emu_mask_full = np.zeros(len(has_2lep), dtype=bool)\n",
    "                            emu_mask_full[indices_selected] = True\n",
    "                            \n",
    "                            # Refresh weights_dict for this charge pass\n",
    "                            weights_dict = {k: v[emu_mask_full] for k, v in base_weights_dict.items()}\n",
    "                            \n",
    "                            leading = lead_all[emu_mask_2lep]\n",
    "                            subleading = sublead_all[emu_mask_2lep]\n",
    "                            met_selected = met[emu_mask_full]\n",
    "\n",
    "                            #        LEPTON SCALE FACTORS (MC ONLY)      \n",
    "                            if not is_data:\n",
    "                                is_lead_ele = (leading.flavor == 11)\n",
    "                                ele_pt = ak.where(is_lead_ele, leading.pt, subleading.pt)\n",
    "                                ele_eta = ak.where(is_lead_ele, leading.eta, subleading.eta)\n",
    "                                ele_sf_nom, ele_sf_err = get_sf_with_uncertainty(ele_eta, ele_pt, ELECTRON_SF_DATA)\n",
    "\n",
    "                                is_lead_mu = (leading.flavor == 13)\n",
    "                                mu_pt = ak.where(is_lead_mu, leading.pt, subleading.pt)\n",
    "                                mu_eta = ak.where(is_lead_mu, leading.eta, subleading.eta)\n",
    "                                mu_tight_nom, mu_tight_err = get_sf_with_uncertainty(mu_eta, mu_pt, MUON_TIGHT_DATA)\n",
    "                                mu_iso_nom, mu_iso_err = get_sf_with_uncertainty(mu_eta, mu_pt, MUON_ISO_DATA)\n",
    "                                \n",
    "                                mu_sf_nom = mu_tight_nom * mu_iso_nom\n",
    "                                mu_sf_up = (mu_tight_nom + mu_tight_err) * (mu_iso_nom + mu_iso_err)\n",
    "                                mu_sf_down = (mu_tight_nom - mu_tight_err) * (mu_iso_nom - mu_iso_err)\n",
    "                                \n",
    "                                ele_sf_up   = ele_sf_nom + ele_sf_err\n",
    "                                ele_sf_down = ele_sf_nom - ele_sf_err\n",
    "\n",
    "                                weights_dict['nominal'] = weights_dict['nominal'] * ele_sf_nom * mu_sf_nom\n",
    "                                weights_dict['trigger_up']   = weights_dict['trigger_up'] * ele_sf_nom * mu_sf_nom\n",
    "                                weights_dict['trigger_down'] = weights_dict['trigger_down'] * ele_sf_nom * mu_sf_nom\n",
    "                                weights_dict['ele_id_up']    = weights_dict['ele_id_up'] * ele_sf_up * mu_sf_nom\n",
    "                                weights_dict['ele_id_down']  = weights_dict['ele_id_down'] * ele_sf_down * mu_sf_nom\n",
    "                                weights_dict['mu_id_up']     = weights_dict['mu_id_up'] * ele_sf_nom * mu_sf_up\n",
    "                                weights_dict['mu_id_down']   = weights_dict['mu_id_down'] * ele_sf_nom * mu_sf_down\n",
    "\n",
    "                            # Record overall preselection yields (OS only to match old cutflow)\n",
    "                            if charge_req == 'OS':\n",
    "                                cutflow['e_mu_preselection'] += len(leading)\n",
    "                                weighted_cutflow['e_mu_preselection'] += float(ak.sum(weights_dict['nominal']))\n",
    "\n",
    "                            #        KINEMATICS      \n",
    "                            masses, ptlls, dphis, mt_higgs, mt_l2_met = cal_kinematic_var(\n",
    "                                leading, subleading, met_selected\n",
    "                            )\n",
    "\n",
    "                            #        PROCESS OPPOSITE SIGN (SIGNAL/TOP/DY)      \n",
    "                            if charge_req == 'OS':\n",
    "                                mjj_before = ak.zeros_like(masses)\n",
    "                                all_true = np.ones(len(masses), dtype=bool)\n",
    "                                \n",
    "                                fill_histograms(\n",
    "                                    'before_cuts', all_true, weights_dict,\n",
    "                                    masses, met_selected.pt, dphis, ptlls,\n",
    "                                    mt_higgs, mt_l2_met, mjj_before,\n",
    "                                    leading.pt, subleading.pt\n",
    "                                )\n",
    "                                \n",
    "                                mjj_selected = ak.fill_none(mjj_full[indices_selected], 0.0)\n",
    "                                \n",
    "                                global_cut_mask, _ = apply_global_cuts(\n",
    "                                    leading, subleading, met_selected, mt_higgs, mt_l2_met, ptlls, masses\n",
    "                                )\n",
    "                                bjet_veto_selected = bjet_veto_full[indices_selected]\n",
    "                                bjet_info_selected = {key: value[indices_selected] for key, value in bjet_info_full.items()}\n",
    "                                \n",
    "                                global_mask_selected = global_cut_mask & bjet_veto_selected\n",
    "                                global_mask_np = ak.to_numpy(global_mask_selected)\n",
    "                                \n",
    "                                cutflow['global_cuts'] += int(np.sum(global_mask_np))\n",
    "                                weighted_cutflow['global_cuts'] += float(ak.sum(weights_dict['nominal'][global_mask_np]))\n",
    "                                \n",
    "                                if np.sum(global_mask_np) == 0:\n",
    "                                    continue\n",
    "                                \n",
    "                                fill_histograms(\n",
    "                                    'global', global_mask_np, weights_dict,\n",
    "                                    masses, met_selected.pt, dphis, ptlls,\n",
    "                                    mt_higgs, mt_l2_met, mjj_selected,\n",
    "                                    leading.pt, subleading.pt\n",
    "                                )\n",
    "                                \n",
    "                                # Jet categories\n",
    "                                isZeroJet = ak.to_numpy(isZeroJet_full[indices_selected])\n",
    "                                isOneJet = ak.to_numpy(isOneJet_full[indices_selected])\n",
    "                                isTwoJet = ak.to_numpy(isTwoJet_full[indices_selected])\n",
    "\n",
    "                                jet_categories = [('0jet', isZeroJet), ('1jet', isOneJet), ('2jet', isTwoJet)]\n",
    "                                \n",
    "                                for jet_name, jet_mask in jet_categories:\n",
    "                                    mask = global_mask_np & jet_mask\n",
    "                                    n_events = int(np.sum(mask))\n",
    "                                    cutflow[jet_name] += n_events\n",
    "                                    weighted_cutflow[jet_name] += float(ak.sum(weights_dict['nominal'][mask]))\n",
    "                                    \n",
    "                                    fill_histograms(\n",
    "                                        jet_name, mask, weights_dict,\n",
    "                                        masses, met_selected.pt, dphis, ptlls,\n",
    "                                        mt_higgs, mt_l2_met, mjj_selected,\n",
    "                                        leading.pt, subleading.pt\n",
    "                                    )\n",
    "                                \n",
    "                                # Signal and Control Regions\n",
    "                                sr_regions = apply_signal_region_cuts(\n",
    "                                    leading, subleading, met_selected, masses, ptlls, mt_higgs,\n",
    "                                    mt_l2_met, isZeroJet_full[indices_selected], isOneJet_full[indices_selected],\n",
    "                                    isTwoJet_full[indices_selected], bjet_veto_selected, mjj_selected\n",
    "                                )\n",
    "                                \n",
    "                                cr_regions = apply_control_region_cuts(\n",
    "                                    leading, subleading, met_selected, masses, ptlls, mt_higgs,\n",
    "                                    mt_l2_met, isZeroJet_full[indices_selected], isOneJet_full[indices_selected],\n",
    "                                    isTwoJet_full[indices_selected], bjet_info_selected, mjj_selected\n",
    "                                )\n",
    "                                \n",
    "                                all_regions = {**sr_regions, **cr_regions}\n",
    "                                \n",
    "                                for region_name, region_mask in all_regions.items():\n",
    "                                    region_mask_np = ak.to_numpy(region_mask)\n",
    "                                    n_events = int(np.sum(region_mask_np))\n",
    "                                    cutflow[region_name] += n_events\n",
    "                                    weighted_cutflow[region_name] += float(ak.sum(weights_dict['nominal'][region_mask_np]))\n",
    "                                    \n",
    "                                    fill_histograms(\n",
    "                                        region_name, region_mask_np, weights_dict,\n",
    "                                        masses, met_selected.pt, dphis, ptlls,\n",
    "                                        mt_higgs, mt_l2_met, mjj_selected,\n",
    "                                        leading.pt, subleading.pt\n",
    "                                    )\n",
    "\n",
    "                            #        PROCESS SAME SIGN (FAKES CR ONLY)      \n",
    "                            elif charge_req == 'SS':\n",
    "                                isZeroJet_ss = ak.to_numpy(isZeroJet_full[indices_selected])\n",
    "                                bjet_veto_ss = bjet_veto_full[indices_selected]\n",
    "                                \n",
    "                                global_cut_mask_ss, _ = apply_global_cuts(\n",
    "                                    leading, subleading, met_selected, mt_higgs, mt_l2_met, ptlls, masses\n",
    "                                )\n",
    "                                \n",
    "                                # Apply Global cuts + 0-jet veto + b-jet veto\n",
    "                                final_ss_mask = global_cut_mask_ss & bjet_veto_ss & isZeroJet_ss\n",
    "                                final_ss_mask_np = ak.to_numpy(final_ss_mask)\n",
    "\n",
    "                                n_events_ss = int(np.sum(final_ss_mask_np))\n",
    "                                if n_events_ss > 0:\n",
    "                                    cutflow['CR_SS_0jet'] += n_events_ss\n",
    "                                    weighted_cutflow['CR_SS_0jet'] += float(ak.sum(weights_dict['nominal'][final_ss_mask_np]))\n",
    "                                    \n",
    "                                    fill_histograms(\n",
    "                                        'CR_SS_0jet', final_ss_mask_np, weights_dict,\n",
    "                                        masses, met_selected.pt, dphis, ptlls,\n",
    "                                        mt_higgs, mt_l2_met, ak.zeros_like(masses),\n",
    "                                        leading.pt, subleading.pt\n",
    "                                    )\n",
    "                    \n",
    "                    # Return results\n",
    "                    return label, stage_histograms, cutflow, weighted_cutflow, None\n",
    "                    \n",
    "                except (OSError, IOError, ValueError) as e:\n",
    "                    if file_attempt < max_file_retries - 1:\n",
    "                        print(f\"  {label}/{file_name}: {type(e).__name__} - Retry {file_attempt+1}/{max_file_retries}\")\n",
    "                        time.sleep(3)\n",
    "                        continue\n",
    "                    else: \n",
    "                        error_msg = f\"{file_name}: {type(e).__name__} after {max_file_retries} attempts - {str(e)[:100]}\"\n",
    "                        return label, initialize_stage_histograms(stage_names, variables_to_plots, VARIATIONS), empty_cutflow, {s: 0.0 for s in cutflow_stages}, error_msg\n",
    "                \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"{file_name}: {type(e).__name__} - {str(e)[:100]}\"\n",
    "                    return label, initialize_stage_histograms(stage_names, variables_to_plots, VARIATIONS), empty_cutflow, {s: 0.0 for s in cutflow_stages}, error_msg\n",
    "            \n",
    "            return label, stage_histograms, cutflow, weighted_cutflow, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_msg = f\"{file_name}: Unexpected error - {str(e)[:100]}\"\n",
    "            return label, initialize_stage_histograms(stage_names, variables_to_plots, VARIATIONS), empty_cutflow, {s: 0.0 for s in cutflow_stages}, error_msg\n",
    "\n",
    "    return processing_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997e242-ef83-4286-a466-577da3f424b7",
   "metadata": {},
   "source": [
    "## Executing the Analysis with Distributed Dask\n",
    "\n",
    "With our `make_processor` function defined, this cell handles the actual computational execution. Because HEP datasets contain millions of events spread across hundreds of ROOT files, running this sequentially on a single machine is impractical. (I tried; it took me around 4-5 hours to run this analysis!)\n",
    "\n",
    "Instead, we utilize **Dask** to parallelize the workload across a distributed cluster. Here is a technical breakdown of the mechanics within this execution block:\n",
    "\n",
    "### 1. Task Distribution via `client.map`\n",
    "Instead of a standard `for` loop, we use `client.map(processing_task, arg_labels, arg_urls, arg_indices, retries=1)`. \n",
    "* Dask takes our `processing_task` function and maps it element-wise across the lists of file URLs and labels. For a deeper look at how this operates, you can review how Dask handles [submitting tasks with client.map](https://distributed.dask.org/en/stable/client.html#distributed.Client.map).\n",
    "* This creates an independent **Task** for every single ROOT file. \n",
    "* The Dask Scheduler instantly distributes these tasks across all available Worker nodes in the cluster. If a worker crashes or experiences a network timeout while reading a remote file, the `retries=1` argument instructs the scheduler to automatically reassign that task to a healthy worker.\n",
    "\n",
    "\n",
    "### 2. Non-Blocking Execution and Futures\n",
    "When `client.map` is called, it does not wait for the computation to finish. Instead, it instantly returns a list of **Futures**. A Future is simply a pointer to a computation that is currently happening in the background on a remote worker, which is how Dask manages [Futures for asynchronous computation](https://distributed.dask.org/en/stable/manage-computation.html). This allows the main Jupyter notebook thread to remain active and responsive while the cluster handles the heavy processing.\n",
    "\n",
    "### 3. Asynchronous Aggregation (`as_completed`)\n",
    "To collect the results, we wrap our list of Futures in Dask's `as_completed()` iterator, an approach designed to handle [task streams efficiently](https://distributed.dask.org/en/stable/api.html#distributed.as_completed).\n",
    "* **Why not use `client.gather()`?** If we waited for all tasks to finish and gathered them simultaneously, we would pull hundreds of dictionaries containing complex histograms into our local RAM all at once. This almost guarantees an Out-Of-Memory (OOM) crash.\n",
    "* **The Streaming Solution:** `as_completed` yields a Future the exact millisecond its specific worker finishes, regardless of the order they were submitted. This prevents the entire pipeline from stalling if one worker gets stuck on a particularly large or slow file.\n",
    "\n",
    "### 4. On-the-Fly Reduction and Memory Management\n",
    "As each worker returns its tuple of results (label, histograms, cutflows), we immediately merge them into our master dictionaries (`hist_data_final` and `cutflow_final`).\n",
    "\n",
    "* Because the `hist` library supports mathematical addition, we can incrementally add the bins of a single file's histogram directly into the master histogram using the `+=` operator.\n",
    "* Once the merge is complete, we call `del result` to immediately destroy the local copy of the worker's output. Combined with `gc.collect()` at the end of the script, this adheres to Dask's best practices for [managing worker memory and avoiding OOM errors](https://distributed.dask.org/en/stable/worker-memory.html). It ensures our local memory footprint remains completely flat throughout the entire execution, no matter how many terabytes of data the cluster processes.\n",
    "\n",
    "### 5. Persistence\n",
    "Finally, the aggregated master histograms are exported to a single ROOT file (`HWW_analysis_output.root`) using `uproot.recreate`. This effectively reduces terabytes of distributed raw data into a few megabytes of dense, statistical distributions, ready for local plotting and limit-setting.\n",
    "\n",
    "If you are new to `Dask` and want to explore plese refer [Dask](https://docs.dask.org/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3215bd4-16dd-46b5-a2bf-904ebd9bc5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PROCESSING START!! \n",
      "Output Directory: /home/cms-jovyan/H-to-WW-NanoAOD-analysis/test/Outputs\n",
      "======================================================================\n",
      "Initializing storage...\n",
      "Preparing file lists...\n",
      "  Data: Validation enabled (1 files)\n",
      "\n",
      "Submitting 9 files to the cluster...\n",
      "Processing and merging results as they arrive...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eeb6ff0eb9f4e188518ba731286fcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing and merging results as they arrive...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m error_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/distributed/client.py:5943\u001b[0m, in \u001b[0;36mas_completed.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5941\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m   5942\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_condition:\n\u001b[0;32m-> 5943\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthread_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_and_raise()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESSING START!! \")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "golden_json_data = None\n",
    "if GOLDEN_JSON_PATH.exists():\n",
    "    with open(GOLDEN_JSON_PATH, 'r') as f:\n",
    "        golden_json_data = json.load(f)\n",
    "# 1. PREPARE PROCESSOR\n",
    "processing_task = make_processor(\n",
    "    golden_json_data=golden_json_data, \n",
    "    sample_info_detailed=sample_info_detailed, \n",
    "    luminosity=LUMINOSITY,                     \n",
    "    run_periods=RUN_PERIODS_2016\n",
    ")\n",
    "\n",
    "# 2. INITIALIZE ACCUMULATORS\n",
    "print(\"Initializing storage...\")\n",
    "hist_data_final = {}\n",
    "cutflow_final = {}\n",
    "weighted_cutflow_final = {}  \n",
    "\n",
    "for label in files.keys():\n",
    "    hist_data_final[label] = initialize_stage_histograms(stage_names, variables_to_plots, VARIATIONS)\n",
    "    cutflow_final[label] = {stage: 0 for stage in cutflow_stages}\n",
    "    weighted_cutflow_final[label] = {stage: 0.0 for stage in cutflow_stages}\n",
    "\n",
    "# 3. PREPARE FILE LISTS\n",
    "arg_labels = []\n",
    "arg_urls = []\n",
    "arg_indices = []\n",
    "\n",
    "print(\"Preparing file lists...\")\n",
    "for label, urls in files.items():\n",
    "    is_data = (label == 'Data')\n",
    "    if is_data and golden_json_data is not None:\n",
    "         print(f\"  {label}: Validation enabled ({len(urls)} files)\")\n",
    "    \n",
    "    for file_idx, file_url in enumerate(urls):\n",
    "        arg_labels.append(label)\n",
    "        arg_urls.append(file_url)\n",
    "        arg_indices.append(file_idx)\n",
    "\n",
    "# 4. SUBMIT TO CLUSTER\n",
    "print(f\"\\nSubmitting {len(arg_urls)} files to the cluster...\")\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "futures = client.map(\n",
    "    processing_task, \n",
    "    arg_labels, \n",
    "    arg_urls, \n",
    "    arg_indices,\n",
    "    retries=1  \n",
    ")\n",
    "\n",
    "# 5. STREAMING MERGE LOOP WITH TQDM\n",
    "print(\"Processing and merging results as they arrive...\")\n",
    "error_count = 0\n",
    "\n",
    "for future in tqdm(as_completed(futures), total=len(futures), unit=\"file\"):\n",
    "    try:\n",
    "        result = future.result()\n",
    "        if not result: \n",
    "            continue\n",
    "        \n",
    "        label, stage_histograms, cutflow, weighted_cutflow, error = result\n",
    "        \n",
    "        if error:\n",
    "            error_count += 1\n",
    "            print(f\" ERROR: {error}\")\n",
    "            continue\n",
    "\n",
    "        # A. Merge Cutflows\n",
    "        if cutflow:\n",
    "            for stage, count in cutflow.items():\n",
    "                cutflow_final[label][stage] += count\n",
    "        \n",
    "        if weighted_cutflow:\n",
    "            for stage, count in weighted_cutflow.items():\n",
    "                weighted_cutflow_final[label][stage] += count\n",
    "        \n",
    "        # B. Merge Histograms\n",
    "        if stage_histograms:\n",
    "            for stage_name, var_dict in stage_histograms.items():\n",
    "                for var_name, syst_dict in var_dict.items():\n",
    "                    for syst_name, hist_obj in syst_dict.items():\n",
    "                        # In-place addition\n",
    "                        hist_data_final[label][stage_name][var_name][syst_name] += hist_obj\n",
    "                        \n",
    "        # Clear memory immediately after merging\n",
    "        del result, stage_histograms, cutflow, weighted_cutflow\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL CLIENT ERROR: {e}\")\n",
    "        error_count += 1\n",
    "\n",
    "elapsed = time.perf_counter() - start_time\n",
    "\n",
    "# 6. SAVE RESULTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"File processing complete. Saving results...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "root_file_path = OUTPUT_DIR / \"HWW_analysis_output.root\"\n",
    "print(f\"\\nSaving histograms to ROOT file: {root_file_path.name}...\")\n",
    "\n",
    "try:\n",
    "    with uproot.recreate(root_file_path) as root_file:\n",
    "        for sample, stages in hist_data_final.items():\n",
    "            for stage, variables in stages.items():\n",
    "                for var_name, variations in variables.items():\n",
    "                    for syst_name, hist_obj in variations.items():\n",
    "                        # Clean name for ROOT: Sample_Stage_Variable_Variation\n",
    "                        hist_name = f\"{sample}_{stage}_{var_name}_{syst_name}\"\n",
    "                        hist_name = hist_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "                        # Write to file\n",
    "                        root_file[hist_name] = hist_obj\n",
    "                        \n",
    "    print(\"  Success! ROOT file saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error saving ROOT file: {e}\")\n",
    "\n",
    "# 7. FINAL REPORT\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'SAMPLE':20s} | {'EVENTS':>12s}\")\n",
    "print(\"-\" * 35)\n",
    "total_events = 0\n",
    "\n",
    "for label in sorted(files.keys()):\n",
    "    events = cutflow_final[label].get('total', 0) if 'total' in cutflow_final[label] else 0\n",
    "    total_events += events\n",
    "    print(f\"{label:20s} | {events:>12,}\")\n",
    "\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'TOTAL':20s} | {total_events:>12,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if error_count > 0:\n",
    "    print(f\"\\n WARNING: {error_count} file(s) failed during processing.\")\n",
    "\n",
    "print(f\"\\nProcessing completed in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "if len(futures) > 0 and elapsed > 0:\n",
    "    print(f\"Average: {elapsed/len(futures):.2f}s per file\")\n",
    "    print(f\"Throughput: {total_events/elapsed:,.0f} events/sec\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Cleanup\n",
    "del futures, arg_urls, arg_labels, arg_indices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360579c-5e5b-4577-a8ea-e498cb5efebe",
   "metadata": {},
   "source": [
    "# Closing the Dask Client\n",
    "\n",
    "Finally, it's always a good practice to cleanly shut down the Dask client we initialized at the start of the notebook and delete the variables associated with it.\n",
    "\n",
    "**But why do we need to do this?**\n",
    "\n",
    "Well, Dask workers reserves a lot of CPU and memory. If we don't explicitly close the client, those resources stay locked up, which can slow down your machine or leave \"zombie\" processes eating up your RAM in the background. Plus, failing to close the client can leave network ports blocked, meaning you might run into annoying port conflict errors the next time you try to run the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a9bd6-4d3e-4fb7-be15-e7502558301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "del client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bdf14-cf9f-4aed-8d3e-f74cf7332fb6",
   "metadata": {},
   "source": [
    "# Outputs\n",
    "\n",
    "With the processing complete, it is time to examine the results of our processing. \n",
    "\n",
    "During the Dask execution, we securely saved our aggregated data to disk:\n",
    "* **Histograms:** Saved in the standard ROOT format at `Outputs/HWW_analysis_output.root`.\n",
    "* **Cutflow Tables:** Saved to the `Outputs/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "035628a4-f6fd-41d5-9db5-2db96bbba193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cms-jovyan/H-to-WW-NanoAOD-analysis/test/Outputs')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1923267e-4661-4214-a37b-d52e14f257f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_cutflow_rows(cutflow_data, custom_stage_info=None, custom_sample_order=None):\n",
    "    \"\"\"\n",
    "    Generates header and rows for the cutflow table.\n",
    "    Returns the data as raw numbers (ints or floats).\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------\n",
    "    # FIXED: Use the global notebook variables if none are passed\n",
    "    # ---------------------------------------------------------\n",
    "    current_stage_info = custom_stage_info if custom_stage_info is not None else stage_info \n",
    "    current_sample_order = custom_sample_order if custom_sample_order is not None else sample_order\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    mc_samples = [s for s in current_sample_order if s != 'Data']\n",
    "    table_stage_names = [s[1] for s in current_stage_info]\n",
    "    stage_keys = [s[0] for s in current_stage_info]\n",
    "    \n",
    "    header = ['Sample'] + table_stage_names\n",
    "    rows = []\n",
    "    \n",
    "    # 1. Fill rows for each sample\n",
    "    for sample in current_sample_order:\n",
    "        if sample not in cutflow_data: \n",
    "            continue\n",
    "            \n",
    "        row = [sample]\n",
    "        for key in stage_keys:\n",
    "            if sample == 'Data' and key == 'total':\n",
    "                val = cutflow_data[sample].get('after_json', 0)\n",
    "            else:\n",
    "                val = cutflow_data[sample].get(key, 0)\n",
    "            row.append(val)\n",
    "        rows.append(row)\n",
    "\n",
    "    # 2. Calculate Total MC row\n",
    "    total_row = ['TOTAL (MC)']\n",
    "    for idx, key in enumerate(stage_keys):\n",
    "        total = sum(cutflow_data[s].get(key, 0) for s in mc_samples if s in cutflow_data)\n",
    "        total_row.append(total)\n",
    "    rows.append(total_row)\n",
    "    \n",
    "    return header, rows\n",
    "\n",
    "def save_cutflows(cutflow_final, weighted_cutflow_final, output_dir):\n",
    "    \"\"\"\n",
    "    Saves the Raw and Weighted cutflow tables to CSV files.\n",
    "    - Raw events -> Cutflow_Raw.csv (Formatted as Integers, no scientific notation)\n",
    "    - Weighted events -> Cutflow_scaled.csv (Formatted to 2 decimal places, no scientific notation)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Save Raw Events (Unweighted)\n",
    "    header_raw, rows_raw = get_cutflow_rows(cutflow_final)\n",
    "    raw_path = output_dir / \"Cutflow_Raw.csv\"\n",
    "    \n",
    "    # Format Raw numbers: Force 0 decimal places \n",
    "    formatted_rows_raw = []\n",
    "    for row in rows_raw:\n",
    "        sample_name = row[0]\n",
    "        formatted = [sample_name] + [f\"{float(val):.0f}\" for val in row[1:]]\n",
    "        formatted_rows_raw.append(formatted)\n",
    "    \n",
    "    try:\n",
    "        with open(raw_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(header_raw)\n",
    "            writer.writerows(formatted_rows_raw)\n",
    "        print(f\"Saved Raw Cutflow to: {raw_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save Raw CSV: {e}\")\n",
    "\n",
    "    # 2. Save Weighted Yields (Scaled)\n",
    "    if weighted_cutflow_final:\n",
    "        header_w, rows_w = get_cutflow_rows(weighted_cutflow_final)\n",
    "        weighted_path = output_dir / \"Cutflow_scaled.csv\"\n",
    "        \n",
    "        # Format Weighted numbers: Force 2 decimal places \n",
    "        formatted_rows_w = []\n",
    "        for row in rows_w:\n",
    "            sample_name = row[0]\n",
    "            formatted = [sample_name] + [f\"{float(val):.2f}\" for val in row[1:]]\n",
    "            formatted_rows_w.append(formatted)\n",
    "\n",
    "        try:\n",
    "            with open(weighted_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(header_w)\n",
    "                writer.writerows(formatted_rows_w)\n",
    "            print(f\"Saved Weighted Cutflow to: {weighted_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save Weighted CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de52a611-c162-4379-ac1b-af8ad56be2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Raw Cutflow to: /home/cms-jovyan/H-to-WW-NanoAOD-analysis/test/Outputs/Cutflow_Raw.csv\n",
      "Saved Weighted Cutflow to: /home/cms-jovyan/H-to-WW-NanoAOD-analysis/test/Outputs/Cutflow_scaled.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_859a8_row0_col0 {\n",
       "  background-color: #ebf3fb;\n",
       "  color: #000000;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col1, #T_859a8_row0_col5, #T_859a8_row0_col8, #T_859a8_row0_col14, #T_859a8_row9_col0, #T_859a8_row9_col2, #T_859a8_row9_col3, #T_859a8_row9_col4, #T_859a8_row9_col6, #T_859a8_row9_col7, #T_859a8_row9_col9, #T_859a8_row9_col10, #T_859a8_row9_col11, #T_859a8_row9_col12, #T_859a8_row9_col13, #T_859a8_row9_col15 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col2 {\n",
       "  background-color: #084b93;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col3, #T_859a8_row0_col6 {\n",
       "  background-color: #1c6bb0;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col4, #T_859a8_row0_col9 {\n",
       "  background-color: #083d7f;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col7 {\n",
       "  background-color: #084285;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col10 {\n",
       "  background-color: #08326e;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col11 {\n",
       "  background-color: #084488;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col12 {\n",
       "  background-color: #7db8da;\n",
       "  color: #000000;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col13 {\n",
       "  background-color: #2171b5;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row0_col15 {\n",
       "  background-color: #74b3d8;\n",
       "  color: #000000;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row1_col0, #T_859a8_row1_col1, #T_859a8_row1_col2, #T_859a8_row1_col4, #T_859a8_row1_col5, #T_859a8_row1_col7, #T_859a8_row1_col8, #T_859a8_row1_col9, #T_859a8_row1_col10, #T_859a8_row1_col11, #T_859a8_row1_col12, #T_859a8_row1_col15, #T_859a8_row2_col0, #T_859a8_row2_col11, #T_859a8_row3_col0, #T_859a8_row4_col11, #T_859a8_row5_col13, #T_859a8_row5_col14, #T_859a8_row6_col0, #T_859a8_row6_col1, #T_859a8_row6_col5, #T_859a8_row6_col8, #T_859a8_row6_col10, #T_859a8_row6_col11, #T_859a8_row6_col12, #T_859a8_row6_col13, #T_859a8_row6_col14, #T_859a8_row6_col15, #T_859a8_row7_col0, #T_859a8_row7_col1, #T_859a8_row7_col2, #T_859a8_row7_col3, #T_859a8_row7_col4, #T_859a8_row7_col6, #T_859a8_row7_col7, #T_859a8_row7_col9, #T_859a8_row7_col10, #T_859a8_row7_col11, #T_859a8_row7_col12, #T_859a8_row7_col14, #T_859a8_row8_col10, #T_859a8_row8_col11, #T_859a8_row8_col14 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row1_col3, #T_859a8_row2_col15, #T_859a8_row3_col15, #T_859a8_row6_col7 {\n",
       "  background-color: #f5fafe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row1_col6, #T_859a8_row2_col12, #T_859a8_row3_col12, #T_859a8_row4_col10, #T_859a8_row8_col9 {\n",
       "  background-color: #f5f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row1_col13, #T_859a8_row2_col10, #T_859a8_row6_col2, #T_859a8_row6_col4, #T_859a8_row6_col9, #T_859a8_row7_col5, #T_859a8_row7_col8, #T_859a8_row7_col13, #T_859a8_row8_col0 {\n",
       "  background-color: #f6faff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row1_col14, #T_859a8_row2_col13, #T_859a8_row4_col6, #T_859a8_row5_col10, #T_859a8_row6_col6, #T_859a8_row8_col1, #T_859a8_row8_col5, #T_859a8_row8_col8 {\n",
       "  background-color: #f3f8fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col1, #T_859a8_row4_col15 {\n",
       "  background-color: #edf4fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col2, #T_859a8_row5_col9 {\n",
       "  background-color: #cde0f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col3 {\n",
       "  background-color: #add0e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col4, #T_859a8_row5_col2 {\n",
       "  background-color: #d3e4f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col5, #T_859a8_row4_col0, #T_859a8_row5_col1 {\n",
       "  background-color: #e6f0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col6, #T_859a8_row3_col4 {\n",
       "  background-color: #a0cbe2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col7, #T_859a8_row4_col4 {\n",
       "  background-color: #cddff1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col8, #T_859a8_row3_col3, #T_859a8_row4_col7 {\n",
       "  background-color: #dfecf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col9, #T_859a8_row4_col8 {\n",
       "  background-color: #eaf2fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row2_col14, #T_859a8_row4_col9, #T_859a8_row8_col2 {\n",
       "  background-color: #f2f7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row3_col1 {\n",
       "  background-color: #5ba3d0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row3_col2 {\n",
       "  background-color: #a8cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row3_col5 {\n",
       "  background-color: #3e8ec4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row3_col6, #T_859a8_row4_col2 {\n",
       "  background-color: #ddeaf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row3_col7 {\n",
       "  background-color: #89bedc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row3_col8, #T_859a8_row3_col9 {\n",
       "  background-color: #3a8ac2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row3_col10 {\n",
       "  background-color: #083b7c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row3_col11 {\n",
       "  background-color: #083a7a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row3_col13, #T_859a8_row5_col11, #T_859a8_row7_col15, #T_859a8_row8_col4 {\n",
       "  background-color: #f1f7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row3_col14 {\n",
       "  background-color: #ccdff1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row4_col1 {\n",
       "  background-color: #c3daee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row4_col3, #T_859a8_row5_col8 {\n",
       "  background-color: #eaf3fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row4_col5 {\n",
       "  background-color: #deebf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row4_col12 {\n",
       "  background-color: #c7dcef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row4_col13 {\n",
       "  background-color: #08509b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row4_col14 {\n",
       "  background-color: #63a8d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row5_col0 {\n",
       "  background-color: #084a91;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row5_col3 {\n",
       "  background-color: #aacfe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row5_col4 {\n",
       "  background-color: #e2edf8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row5_col5 {\n",
       "  background-color: #eef5fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row5_col6 {\n",
       "  background-color: #afd1e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row5_col7 {\n",
       "  background-color: #e1edf8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row5_col12 {\n",
       "  background-color: #3484bf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row5_col15 {\n",
       "  background-color: #2373b6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_859a8_row6_col3 {\n",
       "  background-color: #f4f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row8_col3, #T_859a8_row8_col6 {\n",
       "  background-color: #eff6fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row8_col7 {\n",
       "  background-color: #f2f8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row8_col12 {\n",
       "  background-color: #f0f6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row8_col13 {\n",
       "  background-color: #ecf4fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row8_col15 {\n",
       "  background-color: #d9e8f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_859a8_row9_col1 {\n",
       "  background-color: #08316d;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row9_col5 {\n",
       "  background-color: #083573;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row9_col8 {\n",
       "  background-color: #083471;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_859a8_row9_col14 {\n",
       "  background-color: #1865ac;\n",
       "  color: #f1f1f1;\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_859a8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_859a8_level0_col0\" class=\"col_heading level0 col0\" >Total (Raw)</th>\n",
       "      <th id=\"T_859a8_level0_col1\" class=\"col_heading level0 col1\" >e-Î¼ Preselect</th>\n",
       "      <th id=\"T_859a8_level0_col2\" class=\"col_heading level0 col2\" >Global Cuts</th>\n",
       "      <th id=\"T_859a8_level0_col3\" class=\"col_heading level0 col3\" >0-jet</th>\n",
       "      <th id=\"T_859a8_level0_col4\" class=\"col_heading level0 col4\" >1-jet</th>\n",
       "      <th id=\"T_859a8_level0_col5\" class=\"col_heading level0 col5\" >2-jet</th>\n",
       "      <th id=\"T_859a8_level0_col6\" class=\"col_heading level0 col6\" >SR 0j</th>\n",
       "      <th id=\"T_859a8_level0_col7\" class=\"col_heading level0 col7\" >SR 1j</th>\n",
       "      <th id=\"T_859a8_level0_col8\" class=\"col_heading level0 col8\" >SR 2j</th>\n",
       "      <th id=\"T_859a8_level0_col9\" class=\"col_heading level0 col9\" >CR Top 0j</th>\n",
       "      <th id=\"T_859a8_level0_col10\" class=\"col_heading level0 col10\" >CR Top 1j</th>\n",
       "      <th id=\"T_859a8_level0_col11\" class=\"col_heading level0 col11\" >CR Top 2j</th>\n",
       "      <th id=\"T_859a8_level0_col12\" class=\"col_heading level0 col12\" >CR Tau 0j</th>\n",
       "      <th id=\"T_859a8_level0_col13\" class=\"col_heading level0 col13\" >CR Tau 1j</th>\n",
       "      <th id=\"T_859a8_level0_col14\" class=\"col_heading level0 col14\" >CR Tau 2j</th>\n",
       "      <th id=\"T_859a8_level0_col15\" class=\"col_heading level0 col15\" >CR SS 0j</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Sample</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "      <th class=\"blank col12\" >&nbsp;</th>\n",
       "      <th class=\"blank col13\" >&nbsp;</th>\n",
       "      <th class=\"blank col14\" >&nbsp;</th>\n",
       "      <th class=\"blank col15\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row0\" class=\"row_heading level0 row0\" >Data</th>\n",
       "      <td id=\"T_859a8_row0_col0\" class=\"data row0 col0\" >62,385,800.00</td>\n",
       "      <td id=\"T_859a8_row0_col1\" class=\"data row0 col1\" >320,498.00</td>\n",
       "      <td id=\"T_859a8_row0_col2\" class=\"data row0 col2\" >44,262.00</td>\n",
       "      <td id=\"T_859a8_row0_col3\" class=\"data row0 col3\" >15,240.00</td>\n",
       "      <td id=\"T_859a8_row0_col4\" class=\"data row0 col4\" >15,248.00</td>\n",
       "      <td id=\"T_859a8_row0_col5\" class=\"data row0 col5\" >13,774.00</td>\n",
       "      <td id=\"T_859a8_row0_col6\" class=\"data row0 col6\" >12,023.00</td>\n",
       "      <td id=\"T_859a8_row0_col7\" class=\"data row0 col7\" >10,884.00</td>\n",
       "      <td id=\"T_859a8_row0_col8\" class=\"data row0 col8\" >1,521.00</td>\n",
       "      <td id=\"T_859a8_row0_col9\" class=\"data row0 col9\" >2,218.00</td>\n",
       "      <td id=\"T_859a8_row0_col10\" class=\"data row0 col10\" >16,788.00</td>\n",
       "      <td id=\"T_859a8_row0_col11\" class=\"data row0 col11\" >6,962.00</td>\n",
       "      <td id=\"T_859a8_row0_col12\" class=\"data row0 col12\" >8.00</td>\n",
       "      <td id=\"T_859a8_row0_col13\" class=\"data row0 col13\" >36.00</td>\n",
       "      <td id=\"T_859a8_row0_col14\" class=\"data row0 col14\" >2.00</td>\n",
       "      <td id=\"T_859a8_row0_col15\" class=\"data row0 col15\" >3,225.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row1\" class=\"row_heading level0 row1\" >ggH_HWW</th>\n",
       "      <td id=\"T_859a8_row1_col0\" class=\"data row1 col0\" >15,436.57</td>\n",
       "      <td id=\"T_859a8_row1_col1\" class=\"data row1 col1\" >1,054.98</td>\n",
       "      <td id=\"T_859a8_row1_col2\" class=\"data row1 col2\" >679.11</td>\n",
       "      <td id=\"T_859a8_row1_col3\" class=\"data row1 col3\" >405.73</td>\n",
       "      <td id=\"T_859a8_row1_col4\" class=\"data row1 col4\" >183.56</td>\n",
       "      <td id=\"T_859a8_row1_col5\" class=\"data row1 col5\" >89.82</td>\n",
       "      <td id=\"T_859a8_row1_col6\" class=\"data row1 col6\" >370.58</td>\n",
       "      <td id=\"T_859a8_row1_col7\" class=\"data row1 col7\" >142.04</td>\n",
       "      <td id=\"T_859a8_row1_col8\" class=\"data row1 col8\" >10.18</td>\n",
       "      <td id=\"T_859a8_row1_col9\" class=\"data row1 col9\" >4.86</td>\n",
       "      <td id=\"T_859a8_row1_col10\" class=\"data row1 col10\" >1.79</td>\n",
       "      <td id=\"T_859a8_row1_col11\" class=\"data row1 col11\" >0.24</td>\n",
       "      <td id=\"T_859a8_row1_col12\" class=\"data row1 col12\" >0.06</td>\n",
       "      <td id=\"T_859a8_row1_col13\" class=\"data row1 col13\" >0.30</td>\n",
       "      <td id=\"T_859a8_row1_col14\" class=\"data row1 col14\" >0.04</td>\n",
       "      <td id=\"T_859a8_row1_col15\" class=\"data row1 col15\" >2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row2\" class=\"row_heading level0 row2\" >WW</th>\n",
       "      <td id=\"T_859a8_row2_col0\" class=\"data row2 col0\" >182,245.80</td>\n",
       "      <td id=\"T_859a8_row2_col1\" class=\"data row2 col1\" >17,941.05</td>\n",
       "      <td id=\"T_859a8_row2_col2\" class=\"data row2 col2\" >11,011.40</td>\n",
       "      <td id=\"T_859a8_row2_col3\" class=\"data row2 col3\" >6,642.00</td>\n",
       "      <td id=\"T_859a8_row2_col4\" class=\"data row2 col4\" >3,094.68</td>\n",
       "      <td id=\"T_859a8_row2_col5\" class=\"data row2 col5\" >1,274.71</td>\n",
       "      <td id=\"T_859a8_row2_col6\" class=\"data row2 col6\" >5,837.40</td>\n",
       "      <td id=\"T_859a8_row2_col7\" class=\"data row2 col7\" >2,631.39</td>\n",
       "      <td id=\"T_859a8_row2_col8\" class=\"data row2 col8\" >189.62</td>\n",
       "      <td id=\"T_859a8_row2_col9\" class=\"data row2 col9\" >166.79</td>\n",
       "      <td id=\"T_859a8_row2_col10\" class=\"data row2 col10\" >108.73</td>\n",
       "      <td id=\"T_859a8_row2_col11\" class=\"data row2 col11\" >15.22</td>\n",
       "      <td id=\"T_859a8_row2_col12\" class=\"data row2 col12\" >0.25</td>\n",
       "      <td id=\"T_859a8_row2_col13\" class=\"data row2 col13\" >1.05</td>\n",
       "      <td id=\"T_859a8_row2_col14\" class=\"data row2 col14\" >0.06</td>\n",
       "      <td id=\"T_859a8_row2_col15\" class=\"data row2 col15\" >73.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row3\" class=\"row_heading level0 row3\" >Top_antitop</th>\n",
       "      <td id=\"T_859a8_row3_col0\" class=\"data row3 col0\" >3,480,598.89</td>\n",
       "      <td id=\"T_859a8_row3_col1\" class=\"data row3 col1\" >176,782.09</td>\n",
       "      <td id=\"T_859a8_row3_col2\" class=\"data row3 col2\" >17,454.82</td>\n",
       "      <td id=\"T_859a8_row3_col3\" class=\"data row3 col3\" >2,558.70</td>\n",
       "      <td id=\"T_859a8_row3_col4\" class=\"data row3 col4\" >6,046.73</td>\n",
       "      <td id=\"T_859a8_row3_col5\" class=\"data row3 col5\" >8,849.39</td>\n",
       "      <td id=\"T_859a8_row3_col6\" class=\"data row3 col6\" >2,182.24</td>\n",
       "      <td id=\"T_859a8_row3_col7\" class=\"data row3 col7\" >5,081.78</td>\n",
       "      <td id=\"T_859a8_row3_col8\" class=\"data row3 col8\" >1,001.32</td>\n",
       "      <td id=\"T_859a8_row3_col9\" class=\"data row3 col9\" >1,537.04</td>\n",
       "      <td id=\"T_859a8_row3_col10\" class=\"data row3 col10\" >16,199.82</td>\n",
       "      <td id=\"T_859a8_row3_col11\" class=\"data row3 col11\" >7,239.56</td>\n",
       "      <td id=\"T_859a8_row3_col12\" class=\"data row3 col12\" >0.26</td>\n",
       "      <td id=\"T_859a8_row3_col13\" class=\"data row3 col13\" >1.52</td>\n",
       "      <td id=\"T_859a8_row3_col14\" class=\"data row3 col14\" >0.44</td>\n",
       "      <td id=\"T_859a8_row3_col15\" class=\"data row3 col15\" >55.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row4\" class=\"row_heading level0 row4\" >DY_to_Tau_Tau</th>\n",
       "      <td id=\"T_859a8_row4_col0\" class=\"data row4 col0\" >90,168,168.35</td>\n",
       "      <td id=\"T_859a8_row4_col1\" class=\"data row4 col1\" >83,604.27</td>\n",
       "      <td id=\"T_859a8_row4_col2\" class=\"data row4 col2\" >6,930.77</td>\n",
       "      <td id=\"T_859a8_row4_col3\" class=\"data row4 col3\" >1,468.52</td>\n",
       "      <td id=\"T_859a8_row4_col4\" class=\"data row4 col4\" >3,630.83</td>\n",
       "      <td id=\"T_859a8_row4_col5\" class=\"data row4 col5\" >1,831.42</td>\n",
       "      <td id=\"T_859a8_row4_col6\" class=\"data row4 col6\" >506.13</td>\n",
       "      <td id=\"T_859a8_row4_col7\" class=\"data row4 col7\" >1,510.89</td>\n",
       "      <td id=\"T_859a8_row4_col8\" class=\"data row4 col8\" >111.54</td>\n",
       "      <td id=\"T_859a8_row4_col9\" class=\"data row4 col9\" >69.64</td>\n",
       "      <td id=\"T_859a8_row4_col10\" class=\"data row4 col10\" >221.56</td>\n",
       "      <td id=\"T_859a8_row4_col11\" class=\"data row4 col11\" >16.20</td>\n",
       "      <td id=\"T_859a8_row4_col12\" class=\"data row4 col12\" >4.30</td>\n",
       "      <td id=\"T_859a8_row4_col13\" class=\"data row4 col13\" >42.08</td>\n",
       "      <td id=\"T_859a8_row4_col14\" class=\"data row4 col14\" >1.05</td>\n",
       "      <td id=\"T_859a8_row4_col15\" class=\"data row4 col15\" >369.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row5\" class=\"row_heading level0 row5\" >Fakes</th>\n",
       "      <td id=\"T_859a8_row5_col0\" class=\"data row5 col0\" >926,210,036.68</td>\n",
       "      <td id=\"T_859a8_row5_col1\" class=\"data row5 col1\" >29,008.87</td>\n",
       "      <td id=\"T_859a8_row5_col2\" class=\"data row5 col2\" >9,497.31</td>\n",
       "      <td id=\"T_859a8_row5_col3\" class=\"data row5 col3\" >6,821.83</td>\n",
       "      <td id=\"T_859a8_row5_col4\" class=\"data row5 col4\" >1,910.68</td>\n",
       "      <td id=\"T_859a8_row5_col5\" class=\"data row5 col5\" >764.80</td>\n",
       "      <td id=\"T_859a8_row5_col6\" class=\"data row5 col6\" >5,134.72</td>\n",
       "      <td id=\"T_859a8_row5_col7\" class=\"data row5 col7\" >1,425.01</td>\n",
       "      <td id=\"T_859a8_row5_col8\" class=\"data row5 col8\" >108.35</td>\n",
       "      <td id=\"T_859a8_row5_col9\" class=\"data row5 col9\" >503.69</td>\n",
       "      <td id=\"T_859a8_row5_col10\" class=\"data row5 col10\" >396.60</td>\n",
       "      <td id=\"T_859a8_row5_col11\" class=\"data row5 col11\" >257.79</td>\n",
       "      <td id=\"T_859a8_row5_col12\" class=\"data row5 col12\" >11.90</td>\n",
       "      <td id=\"T_859a8_row5_col13\" class=\"data row5 col13\" >0.00</td>\n",
       "      <td id=\"T_859a8_row5_col14\" class=\"data row5 col14\" >0.00</td>\n",
       "      <td id=\"T_859a8_row5_col15\" class=\"data row5 col15\" >5,008.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row6\" class=\"row_heading level0 row6\" >ggWW</th>\n",
       "      <td id=\"T_859a8_row6_col0\" class=\"data row6 col0\" >8,836.93</td>\n",
       "      <td id=\"T_859a8_row6_col1\" class=\"data row6 col1\" >1,313.42</td>\n",
       "      <td id=\"T_859a8_row6_col2\" class=\"data row6 col2\" >955.39</td>\n",
       "      <td id=\"T_859a8_row6_col3\" class=\"data row6 col3\" >575.11</td>\n",
       "      <td id=\"T_859a8_row6_col4\" class=\"data row6 col4\" >285.49</td>\n",
       "      <td id=\"T_859a8_row6_col5\" class=\"data row6 col5\" >94.80</td>\n",
       "      <td id=\"T_859a8_row6_col6\" class=\"data row6 col6\" >531.57</td>\n",
       "      <td id=\"T_859a8_row6_col7\" class=\"data row6 col7\" >252.90</td>\n",
       "      <td id=\"T_859a8_row6_col8\" class=\"data row6 col8\" >13.55</td>\n",
       "      <td id=\"T_859a8_row6_col9\" class=\"data row6 col9\" >15.51</td>\n",
       "      <td id=\"T_859a8_row6_col10\" class=\"data row6 col10\" >9.30</td>\n",
       "      <td id=\"T_859a8_row6_col11\" class=\"data row6 col11\" >1.04</td>\n",
       "      <td id=\"T_859a8_row6_col12\" class=\"data row6 col12\" >0.02</td>\n",
       "      <td id=\"T_859a8_row6_col13\" class=\"data row6 col13\" >0.08</td>\n",
       "      <td id=\"T_859a8_row6_col14\" class=\"data row6 col14\" >0.00</td>\n",
       "      <td id=\"T_859a8_row6_col15\" class=\"data row6 col15\" >4.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row7\" class=\"row_heading level0 row7\" >Diboson</th>\n",
       "      <td id=\"T_859a8_row7_col0\" class=\"data row7 col0\" >397,290.09</td>\n",
       "      <td id=\"T_859a8_row7_col1\" class=\"data row7 col1\" >1,167.75</td>\n",
       "      <td id=\"T_859a8_row7_col2\" class=\"data row7 col2\" >642.55</td>\n",
       "      <td id=\"T_859a8_row7_col3\" class=\"data row7 col3\" >228.33</td>\n",
       "      <td id=\"T_859a8_row7_col4\" class=\"data row7 col4\" >238.31</td>\n",
       "      <td id=\"T_859a8_row7_col5\" class=\"data row7 col5\" >175.91</td>\n",
       "      <td id=\"T_859a8_row7_col6\" class=\"data row7 col6\" >189.66</td>\n",
       "      <td id=\"T_859a8_row7_col7\" class=\"data row7 col7\" >174.82</td>\n",
       "      <td id=\"T_859a8_row7_col8\" class=\"data row7 col8\" >17.31</td>\n",
       "      <td id=\"T_859a8_row7_col9\" class=\"data row7 col9\" >11.29</td>\n",
       "      <td id=\"T_859a8_row7_col10\" class=\"data row7 col10\" >10.77</td>\n",
       "      <td id=\"T_859a8_row7_col11\" class=\"data row7 col11\" >2.53</td>\n",
       "      <td id=\"T_859a8_row7_col12\" class=\"data row7 col12\" >0.07</td>\n",
       "      <td id=\"T_859a8_row7_col13\" class=\"data row7 col13\" >0.33</td>\n",
       "      <td id=\"T_859a8_row7_col14\" class=\"data row7 col14\" >-0.00</td>\n",
       "      <td id=\"T_859a8_row7_col15\" class=\"data row7 col15\" >229.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row8\" class=\"row_heading level0 row8\" >VG</th>\n",
       "      <td id=\"T_859a8_row8_col0\" class=\"data row8 col0\" >8,029,875.81</td>\n",
       "      <td id=\"T_859a8_row8_col1\" class=\"data row8 col1\" >7,656.66</td>\n",
       "      <td id=\"T_859a8_row8_col2\" class=\"data row8 col2\" >2,142.21</td>\n",
       "      <td id=\"T_859a8_row8_col3\" class=\"data row8 col3\" >1,008.64</td>\n",
       "      <td id=\"T_859a8_row8_col4\" class=\"data row8 col4\" >724.10</td>\n",
       "      <td id=\"T_859a8_row8_col5\" class=\"data row8 col5\" >409.47</td>\n",
       "      <td id=\"T_859a8_row8_col6\" class=\"data row8 col6\" >797.68</td>\n",
       "      <td id=\"T_859a8_row8_col7\" class=\"data row8 col7\" >453.94</td>\n",
       "      <td id=\"T_859a8_row8_col8\" class=\"data row8 col8\" >41.05</td>\n",
       "      <td id=\"T_859a8_row8_col9\" class=\"data row8 col9\" >36.48</td>\n",
       "      <td id=\"T_859a8_row8_col10\" class=\"data row8 col10\" >29.05</td>\n",
       "      <td id=\"T_859a8_row8_col11\" class=\"data row8 col11\" >2.14</td>\n",
       "      <td id=\"T_859a8_row8_col12\" class=\"data row8 col12\" >0.69</td>\n",
       "      <td id=\"T_859a8_row8_col13\" class=\"data row8 col13\" >2.71</td>\n",
       "      <td id=\"T_859a8_row8_col14\" class=\"data row8 col14\" >-0.00</td>\n",
       "      <td id=\"T_859a8_row8_col15\" class=\"data row8 col15\" >1,015.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_859a8_level0_row9\" class=\"row_heading level0 row9\" >TOTAL (MC)</th>\n",
       "      <td id=\"T_859a8_row9_col0\" class=\"data row9 col0\" >1,028,492,489.12</td>\n",
       "      <td id=\"T_859a8_row9_col1\" class=\"data row9 col1\" >318,529.09</td>\n",
       "      <td id=\"T_859a8_row9_col2\" class=\"data row9 col2\" >49,313.56</td>\n",
       "      <td id=\"T_859a8_row9_col3\" class=\"data row9 col3\" >19,708.86</td>\n",
       "      <td id=\"T_859a8_row9_col4\" class=\"data row9 col4\" >16,114.37</td>\n",
       "      <td id=\"T_859a8_row9_col5\" class=\"data row9 col5\" >13,490.33</td>\n",
       "      <td id=\"T_859a8_row9_col6\" class=\"data row9 col6\" >15,549.98</td>\n",
       "      <td id=\"T_859a8_row9_col7\" class=\"data row9 col7\" >11,672.78</td>\n",
       "      <td id=\"T_859a8_row9_col8\" class=\"data row9 col8\" >1,492.90</td>\n",
       "      <td id=\"T_859a8_row9_col9\" class=\"data row9 col9\" >2,345.30</td>\n",
       "      <td id=\"T_859a8_row9_col10\" class=\"data row9 col10\" >16,977.62</td>\n",
       "      <td id=\"T_859a8_row9_col11\" class=\"data row9 col11\" >7,534.72</td>\n",
       "      <td id=\"T_859a8_row9_col12\" class=\"data row9 col12\" >17.56</td>\n",
       "      <td id=\"T_859a8_row9_col13\" class=\"data row9 col13\" >48.07</td>\n",
       "      <td id=\"T_859a8_row9_col14\" class=\"data row9 col14\" >1.59</td>\n",
       "      <td id=\"T_859a8_row9_col15\" class=\"data row9 col15\" >6,757.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1277eff410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_cutflows(\n",
    "    cutflow_final=cutflow_final, \n",
    "    weighted_cutflow_final=weighted_cutflow_final, \n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "df = pd.read_csv(OUTPUT_DIR / \"Cutflow_scaled.csv\", index_col=0)\n",
    "rows_to_highlight = [df.index[0], df.index[-1]]\n",
    "styled_df = df.style.format(\"{:,.2f}\") \\\n",
    "    .background_gradient(cmap=\"Blues\", axis=0) \\\n",
    "    .set_properties(\n",
    "        subset=pd.IndexSlice[rows_to_highlight, :], \n",
    "        **{'font-weight': 'bold'}\n",
    "    )\n",
    "\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373870a-2d77-488a-a6f5-2475c93df059",
   "metadata": {},
   "source": [
    "## Loading Processed Data from ROOT\n",
    "Once the processing is done, it is generally not a good idea to rely solely on the results provided by the main processing loopâwhich in our case is `hist_data_final`. If the server unexpectedly crashes or we disconnect from it, we could lose all our progress. \n",
    "\n",
    "Therefore, we will use the `HWW_analysis_output.root` file that we created during the processing stage to load the dictionary containing all our information.\n",
    "\n",
    "**Why is this the preferred workflow?**\n",
    "* **Persistence:** The ROOT file acts as a permanent snapshot of our results. We can close the notebook, come back days later, and pick up exactly where we left off without rerunning the Dask cluster.\n",
    "* **Memory Efficiency:** Instead of keeping every single histogram in the active Python RAM, we can use `uproot` to surgically extract only the specific regions or variables we need for a particular plot.\n",
    "* **Portability:** This output file is relatively small (megabytes instead of the terabytes of raw data we started with), making it easy to share with collaborators or move to a local machine for final styling and plotting.\n",
    "\n",
    "In the following cell, we use `uproot` to open this file and reconstruct our nested dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e43521ff-78d7-4a4b-a299-5cbf17639e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring histograms from: HWW_analysis_output.root\n",
      "Successfully restored data for 9 samples.\n"
     ]
    }
   ],
   "source": [
    "def restore_histograms(sample_list, stage_names, vars_dict, variations, root_file_path):\n",
    "    \"\"\"\n",
    "    Reconstructs the full nested dictionary of histograms from a saved ROOT file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_list : list\n",
    "        List of sample names (e.g., files.keys()) to initialize.\n",
    "    stage_names : list\n",
    "        List of analysis stages (e.g., Config.stage_names).\n",
    "    vars_dict : dict\n",
    "        Dictionary of variables (e.g., Plots_config.variables_to_plots).\n",
    "    variations : list\n",
    "        List of systematic variations (e.g., Config.VARIATIONS).\n",
    "    root_file_path : str or Path\n",
    "        Path to the .root file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The fully populated hist_data dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = Path(root_file_path)\n",
    "    if not path.exists():\n",
    "        print(f\"CRITICAL: ROOT file not found at {path}\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Restoring histograms from: {path.name}\")\n",
    "    \n",
    "    hist_data = {}\n",
    "    \n",
    "    # Open file once\n",
    "    with uproot.open(path) as file:\n",
    "        # Loop through known structure to find matching keys\n",
    "        for sample in sample_list:\n",
    "            # 1. Initialize empty structure for this sample\n",
    "            hist_data[sample] = initialize_stage_histograms(stage_names, vars_dict, variations)\n",
    "            \n",
    "            # 2. Fill with data from ROOT file\n",
    "            for stage in stage_names:\n",
    "                for var in vars_dict.keys():\n",
    "                    for syst in variations:\n",
    "                        # Reconstruct the key name used during saving\n",
    "                        # Format: Sample_Stage_Variable_Variation\n",
    "                        hist_name = f\"{sample}_{stage}_{var}_{syst}\"\n",
    "                        hist_name = hist_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "                        \n",
    "                        if hist_name in file:\n",
    "                            # .to_hist() converts Uproot object back to boost_histogram\n",
    "                            hist_data[sample][stage][var][syst] = file[hist_name].to_hist()\n",
    "                            \n",
    "    print(f\"Successfully restored data for {len(hist_data)} samples.\")\n",
    "    return hist_data\n",
    "\n",
    "hist_data_final = restore_histograms(\n",
    "    sample_list=sorted(files.keys()),       \n",
    "    stage_names=stage_names,\n",
    "    vars_dict=variables_to_plots,\n",
    "    variations=VARIATIONS,\n",
    "    root_file_path=OUTPUT_DIR / \"HWW_analysis_output.root\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ddb73-146e-4ef3-a44d-628fc4046892",
   "metadata": {},
   "source": [
    "We can use `type()` to confirm that our results were successfully aggregated into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff28f6d3-8c38-496b-a20e-68b5d5ef5357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hist_data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce65cf-b139-4cc0-aa50-bfefe80d5798",
   "metadata": {},
   "source": [
    "# Plots\n",
    "We will generate two distinct types of plots:\n",
    "\n",
    "1. **Superimposed Plots (Shape Analysis)**\\\n",
    "In these plots, we normalize the area of every process to `1.0` and overlay them on the same axes.\n",
    "> **Why?**\\\n",
    "> This allows us to ignore how \"large\" a process is and focus purely on its shape. It is the best way to see how our selection cuts change the kinematic distributions of our signal compared to the background at different stages of the analysis.\n",
    "\n",
    "2. **Stacked Plots (Yield and Region Analysis)**\\\n",
    "Here, we stack the backgrounds on top of each other and layer our signal (and eventually our Data) on top.\n",
    ">**Why?**\\\n",
    ">It allows us to visualize the total yield (number of events) and see exactly how much signal is out above the background. This is where we truly \"see\" the Signal and Control Regions we defined earlier and can judge how well our simulations match the reality of the detector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc7112-6494-4cb0-9910-896b1a954ec5",
   "metadata": {},
   "source": [
    "## Superimposed plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25146c7c-44b6-421a-9c2d-e91aad336c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_stage_comparison(variable, var_props, hist_data_all, output_dir=None):\n",
    "    \n",
    "#     stages = ['before_cuts', 'global', '0jet', '1jet', '2jet']\n",
    "#     stage_labels = [r'Pre-Selection ', r'Global cuts', r'0-jet', r'1-jet', r'$\\geq$2-jet']\n",
    "    \n",
    "#     # Create figure\n",
    "#     fig, axes = plt.subplots(1, 5, figsize=(30, 7)) \n",
    "    \n",
    "#     var_map = {\n",
    "#         'mass': 'mass', 'met': 'met', 'dphi': 'dphi', 'ptll': 'ptll',\n",
    "#         'mt_higgs': 'mt_higgs', 'mt_l2_met': 'mt_l2_met', 'mjj': 'mjj'\n",
    "#     }\n",
    "    \n",
    "#     hist_key = var_map.get(variable, variable)\n",
    "    \n",
    "#     xlabel = VAR_LABELS.get(variable, var_props.label if hasattr(var_props, 'label') else variable)\n",
    "#     xlim = (var_props.edges[0], var_props.edges[-1]) if hasattr(var_props, 'edges') else None\n",
    "\n",
    "#     for idx, (stage, stage_label) in enumerate(zip(stages, stage_labels)):\n",
    "#         ax = axes[idx]\n",
    "#         has_data = False\n",
    "        \n",
    "#         for sample in files.keys():\n",
    "#             if sample not in hist_data_all: continue\n",
    "#             if stage not in hist_data_all[sample]: continue\n",
    "#             if hist_key not in hist_data_all[sample][stage]: continue\n",
    "            \n",
    "#             hist_variations = hist_data_all[sample][stage][hist_key]\n",
    "            \n",
    "#             if 'nominal' not in hist_variations:\n",
    "#                 continue\n",
    "                \n",
    "#             hist_obj = hist_variations['nominal']\n",
    "            \n",
    "#             try:\n",
    "#                 if hasattr(hist_obj, 'to_numpy'):\n",
    "#                     values, edges = hist_obj.to_numpy() \n",
    "#                 else:\n",
    "#                     values = hist_obj.values()\n",
    "#                     edges = hist_obj.axes[0].edges\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Skipping {sample} {stage}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             total = np.sum(values)\n",
    "#             if total == 0: continue\n",
    "            \n",
    "#             has_data = True\n",
    "\n",
    "#             is_sig = False\n",
    "#             color = 'black'\n",
    "#             if 'SAMPLES' in globals() and sample in SAMPLES:\n",
    "#                 is_sig = SAMPLES[sample].get(\"is_signal\", False)\n",
    "#                 color = SAMPLES[sample][\"color\"]\n",
    "            \n",
    "#             if \"DATA\" in sample.upper():\n",
    "#                 continue            \n",
    "#             else:\n",
    "#                 lw = 2 if is_sig else 1\n",
    "#                 zord = 10 if is_sig else 1\n",
    "                \n",
    "#                 hep.histplot(values, bins=edges, density=True, \n",
    "#                              histtype='step', \n",
    "#                              linewidth=lw,\n",
    "#                              label=sample,\n",
    "#                              color=color,\n",
    "#                              ax=ax, zorder=zord)\n",
    "\n",
    "#         # STYLING \n",
    "#         if has_data:\n",
    "#             hep.cms.label(ax=ax, loc=0, data=True, label=\"Open Data\", \n",
    "#                           lumi=16.39, fontsize=16)\n",
    "#             ax.set_title(stage_label, pad=20, fontsize=18, fontweight='bold')\n",
    "#             ax.set_xlabel(xlabel, fontsize=18) \n",
    "            \n",
    "#             if idx == 0:\n",
    "#                 ax.set_ylabel(r\"Random Units\", fontsize=16)\n",
    "            \n",
    "#             if xlim: \n",
    "#                 ax.set_xlim(xlim)\n",
    "                \n",
    "#             ax.grid(True, linestyle=':', alpha=0.5)\n",
    "#             # if idx == 4: \n",
    "#             ax.legend(loc='upper right', fontsize=12, frameon=False)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # SAVING FILES\n",
    "#     if output_dir:\n",
    "#         out_file = output_dir / f\"{variable}_stages.png\"\n",
    "#         fig.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "#         print(f\"Saved plot: {out_file.name}\")\n",
    "\n",
    "#     return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d905f2ec-b58d-45ac-b535-6e2784ae4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Saving plots to: {PLOTS_DIR / \"Kinematics\"}\")\n",
    "# for var_name, var_props in variables_to_plots.items():\n",
    "#     print(f\"Processing {var_name}...\")\n",
    "#     fig = plot_stage_comparison(var_name, var_props, hist_data_final, output_dir=PLOTS_DIR / \"Kinematics\")\n",
    "    \n",
    "#     plt.show()\n",
    "#     plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e8bda-7629-45a0-867a-8b9e3653d87f",
   "metadata": {},
   "source": [
    "## Stacked plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d81b0a-37e0-4bac-a0cb-24a764fd28cd",
   "metadata": {},
   "source": [
    "### Configuring the Stacked Plots\n",
    "\n",
    "This `PLOT_SETTINGS` dictionary acts as our master blueprint. It organizes our output into the three distinct physics regions we defined earlier: the **Signal Region**, the **Top Control Region**, and the **DY-$\\tau\\tau$ Control Region**.\n",
    "\n",
    "There are a few important experimental design choices embedded in this dictionary:\n",
    "\n",
    "1. **Blinding the Signal**\\\n",
    "Notice the `plot_data` flag. It is set to `True` for our Control Regions, but strictly `False` for our Signal Region. \n",
    "\n",
    "2. **Dynamic Kinematic Axes**\\\n",
    "You will also notice custom `xlim` and `ylim` ranges for the same variable depending on the region. By defining these ranges here, our plotting function will dynamically zoom in on the relevant phase space for each specific control region, ensuring our distributions are perfectly framed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a388c30d-e955-416b-a659-60ce99133127",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_SETTINGS = {\n",
    "    # GROUP 1: SIGNAL REGION \n",
    "    \"Signal_Region\": {\n",
    "        \"plot_data\": False, \n",
    "        \"stages\": [('SR_0jet', 'SR 0j'), ('SR_1jet', 'SR 1j'), ('SR_2jet', 'SR 2j')],\n",
    "        \"variables\": {\n",
    "            \"mass\":          {\"log\": True,  \"xlim\": (12, 200),  \"ylim\": (0.01, 5000)},\n",
    "            \"ptll\":          {\"log\": True,  \"xlim\": (30, 200),  \"ylim\": (0.01, 5000)},\n",
    "            \"met\":           {\"log\": True,  \"xlim\": (20, 200),  \"ylim\": (0.01, 5000)},\n",
    "            \"mt_higgs\":      {\"log\": True,  \"xlim\": (60, 300),  \"ylim\": None},\n",
    "            \"mt_l2_met\":     {\"log\": True,  \"xlim\": (30, 140),  \"ylim\": None},\n",
    "            \"mjj\":           {\"log\": True,  \"xlim\": (0, 500),   \"ylim\": None}, \n",
    "            \"dphi\":          {\"log\": True,  \"xlim\": (0, 3.14),  \"ylim\": (0.01, 1000)},\n",
    "            \"leading_pt\":    {\"log\": True,  \"xlim\": (25, 200),  \"ylim\": None},\n",
    "            \"subleading_pt\": {\"log\": True,  \"xlim\": (10, 200),  \"ylim\": None},\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # GROUP 2: TOP CONTROL REGION \n",
    "    \"Control_Region_Top\": {\n",
    "        \"plot_data\": True,\n",
    "        \"stages\": [('CR_top_0jet', 'Top 0j'), ('CR_top_1jet', 'Top 1j'), ('CR_top_2jet', 'Top 2j')],\n",
    "        \"variables\": {\n",
    "            \"mass\":          {\"log\": True, \"xlim\": (50, 200),  \"ylim\": (1, 5000)},\n",
    "            \"ptll\":          {\"log\": True, \"xlim\": (30, 200),  \"ylim\": (1, 2000)},\n",
    "            \"met\":           {\"log\": True, \"xlim\": (20, 200),  \"ylim\": None},\n",
    "            \"mt_higgs\":      {\"log\": True, \"xlim\": (60, 300),  \"ylim\": (1, 5000)},\n",
    "            \"mt_l2_met\":     {\"log\": True, \"xlim\": (30, 150),  \"ylim\": None},\n",
    "            \"mjj\":           {\"log\": True, \"xlim\": (0, 500),   \"ylim\": None},\n",
    "            \"dphi\":          {\"log\": True, \"xlim\": (0, 3.14),  \"ylim\": (1, 1000)},\n",
    "            \"leading_pt\":    {\"log\": True, \"xlim\": (25, 200),  \"ylim\": None},\n",
    "            \"subleading_pt\": {\"log\": True, \"xlim\": (10, 200),  \"ylim\": None},\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # GROUP 3: TAU CONTROL REGION \n",
    "    \"Control_Region_Tau\": {\n",
    "        \"plot_data\": True,\n",
    "        \"stages\": [('CR_tau_0jet', r'DY-$\\tau\\tau$ 0j'), ('CR_tau_1jet', r'DY-$\\tau\\tau$ 1j'), ('CR_tau_2jet', r'DY-$\\tau\\tau$ 2j')],\n",
    "        \"variables\": {\n",
    "            \"mass\":          {\"log\": True, \"xlim\": (40, 60),   \"ylim\": (0.01, 1000)},\n",
    "            \"ptll\":          {\"log\": True, \"xlim\": (30, 100),  \"ylim\": (0.01, 1000)},\n",
    "            \"met\":           {\"log\": True, \"xlim\": (20, 100),   \"ylim\": (0.01, 1000)},\n",
    "            \"mt_higgs\":      {\"log\": True, \"xlim\": (0, 60),    \"ylim\": (0.01, 1000)},\n",
    "            \"mt_l2_met\":     {\"log\": True, \"xlim\": (30, 50),   \"ylim\": (0.01, 1000)},\n",
    "            \"mjj\":           {\"log\": True, \"xlim\": (0, 500),   \"ylim\": None},\n",
    "            \"dphi\":          {\"log\": True, \"xlim\": (1.5, 3.14),  \"ylim\": (0.01, 1000)},\n",
    "            \"leading_pt\":    {\"log\": True, \"xlim\": (30, 80),  \"ylim\": (0.01, 1000)},\n",
    "            \"subleading_pt\": {\"log\": True, \"xlim\": (10, 30),   \"ylim\": None},\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de487ebc-51eb-4b09-92d3-76be7e2d31ea",
   "metadata": {},
   "source": [
    "## Safely Extracting Histogram Data\n",
    "\n",
    "To access histograms from our nested dictionary, we use the `get_histogram_data` function. It navigates the hierarchy  \n",
    "`Sample â Stage â Variable â Variation`  \n",
    "and safely returns `None` if a requested combination does not exist, preventing `KeyError` exceptions during plotting (e.g., when data is unavailable in a blinded region).\n",
    "\n",
    "The function also extracts the essential components needed for visualization:\n",
    "\n",
    "- **Bin values**\n",
    "- **Bin edges**\n",
    "- **Statistical variances**\n",
    "\n",
    "Since the histograms were initialized with weight storage, calling `.variances()` directly returns the sum of squared weights. This allows us to compute statistically correct uncertainty bands for the final plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63b5126f-3e1d-4942-8249-22212feb7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram_data(hist_data, sample, stage, variable, variation='nominal'):\n",
    "    if sample not in hist_data: return None, None, None\n",
    "    if stage not in hist_data[sample]: return None, None, None\n",
    "    if variable not in hist_data[sample][stage]: return None, None, None\n",
    "    \n",
    "    vars_dict = hist_data[sample][stage][variable]\n",
    "    if variation not in vars_dict: return None, None, None\n",
    "        \n",
    "    h = vars_dict[variation]\n",
    "    \n",
    "    try:\n",
    "        return h.values(), h.variances(), h.axes[0].edges\n",
    "    except:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0fcac3-8d2a-4c32-8a75-ac20f491253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stacked_plots(variable, hist_data_all, output_dir=\"plots\"):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     xlabel = VAR_LABELS.get(variable, variable)\n",
    "    \n",
    "#     backgrounds = [s for s in SAMPLES if not SAMPLES[s]['is_signal'] and s != 'Data']\n",
    "#     backgrounds.sort(key=lambda s: SAMPLES[s].get(\"stack_order\", 0))\n",
    "#     signal = next((s for s in SAMPLES if SAMPLES[s]['is_signal']), None)\n",
    "#     data_sample = 'Data'\n",
    "\n",
    "#     syst_sources = ['trigger', 'ele_id', 'mu_id'] \n",
    "\n",
    "#     for region_name, config in PLOT_SETTINGS.items():\n",
    "#         var_config = config['variables'].get(variable)\n",
    "#         if not var_config: continue\n",
    "\n",
    "#         print(f\"Plotting {variable} in {region_name}...\")\n",
    "        \n",
    "#         stages = config['stages']\n",
    "#         use_log = var_config.get('log', False)\n",
    "#         set_xlim = var_config.get('xlim', None)\n",
    "#         set_ylim = var_config.get('ylim', None)\n",
    "        \n",
    "#         is_signal_region = \"Signal\" in region_name\n",
    "#         plot_data = False if is_signal_region else config.get('plot_data', True)\n",
    "\n",
    "#         # CANVAS SETUP\n",
    "#         if plot_data:\n",
    "#             fig, axes = plt.subplots(2, 3, figsize=(30, 12), \n",
    "#                                      gridspec_kw={'height_ratios': [3, 1], 'hspace': 0.08, 'wspace': 0.25},\n",
    "#                                      sharex='col')\n",
    "#             row_axes = axes[0]\n",
    "#             ratio_axes = axes[1]\n",
    "#         else:\n",
    "#             fig, axes = plt.subplots(1, 3, figsize=(30, 9), \n",
    "#                                      gridspec_kw={'wspace': 0.25},\n",
    "#                                      sharex='col')\n",
    "#             row_axes = axes\n",
    "#             ratio_axes = [None] * 3\n",
    "\n",
    "#         for col_idx, (stage_key, stage_label) in enumerate(stages):\n",
    "#             ax_main = row_axes[col_idx]\n",
    "#             ax_ratio = ratio_axes[col_idx]\n",
    "            \n",
    "#             # GET NOMINAL DATA \n",
    "#             stack_vals = []\n",
    "#             stack_colors = []\n",
    "#             stack_labels = []\n",
    "#             stack_variances = [] \n",
    "#             edges = None\n",
    "            \n",
    "#             total_nominal = None\n",
    "#             syst_stacks = {source: {'up': None, 'down': None} for source in syst_sources}\n",
    "\n",
    "#             for s in backgrounds:\n",
    "#                 vals, vars_sq, e = get_histogram_data(hist_data_all, s, stage_key, variable, 'nominal')\n",
    "                \n",
    "#                 if vals is not None:\n",
    "#                     if edges is None: \n",
    "#                         edges = e\n",
    "#                         nbins = len(vals)\n",
    "#                         total_nominal = np.zeros(nbins)\n",
    "#                         for src in syst_sources:\n",
    "#                             syst_stacks[src]['up'] = np.zeros(nbins)\n",
    "#                             syst_stacks[src]['down'] = np.zeros(nbins)\n",
    "\n",
    "#                     stack_vals.append(vals)\n",
    "#                     stack_variances.append(vars_sq) # Add stat variance\n",
    "#                     stack_colors.append(SAMPLES[s].get(\"color\", \"gray\"))\n",
    "#                     stack_labels.append(s)\n",
    "                    \n",
    "#                     # Add to totals\n",
    "#                     total_nominal += vals\n",
    "                    \n",
    "#                     # Accumulate Systematics for this background\n",
    "#                     for src in syst_sources:\n",
    "#                         v_up, _, _ = get_histogram_data(hist_data_all, s, stage_key, variable, f'{src}_up')\n",
    "#                         v_dn, _, _ = get_histogram_data(hist_data_all, s, stage_key, variable, f'{src}_down')\n",
    "                        \n",
    "#                         if v_up is None: v_up = vals\n",
    "#                         if v_dn is None: v_dn = vals\n",
    "                            \n",
    "#                         syst_stacks[src]['up'] += v_up\n",
    "#                         syst_stacks[src]['down'] += v_dn\n",
    "\n",
    "#             if edges is None:\n",
    "#                 ax_main.text(0.5, 0.5, \"No Data\", ha='center', transform=ax_main.transAxes)\n",
    "#                 continue\n",
    "                \n",
    "#             # Stats Uncertainty\n",
    "#             total_stat_variance = np.sum(stack_variances, axis=0)\n",
    "            \n",
    "#             # Systematic Uncertainty\n",
    "#             total_syst_variance = np.zeros_like(total_nominal)\n",
    "            \n",
    "#             for src in syst_sources:\n",
    "#                 diff_up = np.abs(syst_stacks[src]['up'] - total_nominal)\n",
    "#                 diff_dn = np.abs(syst_stacks[src]['down'] - total_nominal)\n",
    "                \n",
    "#                 max_diff = np.maximum(diff_up, diff_dn)\n",
    "#                 total_syst_variance += max_diff**2\n",
    "                \n",
    "#             # Total Error (Stat + Syst)\n",
    "#             total_err = np.sqrt(total_stat_variance + total_syst_variance)\n",
    "\n",
    "#             # PLOTTING      \n",
    "            \n",
    "#             # A. Stack\n",
    "#             if stack_vals:\n",
    "#                 hep.histplot(stack_vals, bins=edges, stack=True, histtype='fill',\n",
    "#                              color=stack_colors, label=stack_labels, ax=ax_main)\n",
    "                \n",
    "#                 # Outline\n",
    "#                 hep.histplot(total_nominal, bins=edges, histtype='step', color='black', linewidth=1, ax=ax_main)\n",
    "                \n",
    "#                 # DRAW UNCERTAINTY BAND\n",
    "#                 band_low = np.append(total_nominal - total_err, (total_nominal - total_err)[-1])\n",
    "#                 band_high = np.append(total_nominal + total_err, (total_nominal + total_err)[-1])\n",
    "                \n",
    "#                 ax_main.fill_between(edges, band_low, band_high, step='post', \n",
    "#                                      facecolor='none', edgecolor='gray', \n",
    "#                                      label='Stat+Syst Unc.', hatch='////', zorder=2)\n",
    "\n",
    "#             # B. Signal\n",
    "#             if signal:\n",
    "#                 sig_vals, _, _ = get_histogram_data(hist_data_all, signal, stage_key, variable, 'nominal')\n",
    "#                 if sig_vals is not None:\n",
    "#                     sig_scale = 10 if is_signal_region else 1\n",
    "#                     sig_lbl = f\"{signal} (x{sig_scale})\" if sig_scale > 1 else signal\n",
    "#                     hep.histplot(sig_vals * sig_scale, bins=edges, histtype='step',\n",
    "#                                  color=SAMPLES[signal].get(\"color\", \"red\"), \n",
    "#                                  linewidth=3, label=sig_lbl, ax=ax_main)\n",
    "\n",
    "#             # C. Data\n",
    "#             data_vals = None\n",
    "#             if plot_data:\n",
    "#                 data_vals, _, _ = get_histogram_data(hist_data_all, data_sample, stage_key, variable, 'nominal')\n",
    "#                 if data_vals is not None:\n",
    "#                     yerr = np.sqrt(data_vals); yerr[data_vals == 0] = 0\n",
    "#                     hep.histplot(data_vals, bins=edges, histtype='errorbar', color='black', \n",
    "#                                  label='Data', yerr=yerr, marker='o', markersize=5, ax=ax_main, zorder=10)\n",
    "\n",
    "#             # D. Ratio Plot\n",
    "#             if plot_data and data_vals is not None and total_nominal is not None:\n",
    "#                 safe_denom = np.where(total_nominal == 0, 1e-9, total_nominal)\n",
    "#                 ratio = data_vals / safe_denom\n",
    "#                 ratio_stat_err = np.abs(np.sqrt(data_vals) / safe_denom) \n",
    "                \n",
    "#                 # Points\n",
    "#                 hep.histplot(ratio, bins=edges, histtype='errorbar', yerr=ratio_stat_err,\n",
    "#                              color='black', marker='o', markersize=4, ax=ax_ratio)\n",
    "                \n",
    "#                 rel_err = total_err / safe_denom\n",
    "#                 rel_err[total_nominal == 0] = 0 \n",
    "                \n",
    "#                 ratio_band_low = np.append(1.0 - rel_err, (1.0 - rel_err)[-1])\n",
    "#                 ratio_band_high = np.append(1.0 + rel_err, (1.0 + rel_err)[-1])\n",
    "                \n",
    "#                 ax_ratio.fill_between(edges, ratio_band_low, ratio_band_high, step='post',\n",
    "#                                       facecolor='gray', alpha=0.3, zorder=1) # Solid gray or hatched\n",
    "                \n",
    "#                 ax_ratio.axhline(1, color='gray', linestyle='--')\n",
    "#                 ax_ratio.set_ylim(0.5, 1.5)\n",
    "#                 ax_ratio.set_ylabel(\"Data / Pred.\", fontsize=16)\n",
    "#                 ax_ratio.set_xlabel(xlabel, fontsize=20)\n",
    "#                 ax_ratio.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "#             #       STYLING      \n",
    "#             hep.cms.label(ax=ax_main, loc=0, data=True, label=\"Open Data\", lumi=16.1, fontsize=20)\n",
    "#             ax_main.text(0.05, 0.92, stage_label, transform=ax_main.transAxes, fontsize=22, fontweight='bold', va='top')\n",
    "#             ax_main.set_ylabel(\"Events / Bin\", fontsize=20)\n",
    "            \n",
    "#             if use_log:\n",
    "#                 ax_main.set_yscale('log')\n",
    "#                 max_val = np.max(total_nominal) if total_nominal is not None else 1\n",
    "#                 ax_main.set_ylim(0.1, max_val * 500)\n",
    "#             else:\n",
    "#                 max_val = np.max(total_nominal) if total_nominal is not None else 1\n",
    "#                 ax_main.set_ylim(0, max_val * 1.5)\n",
    "\n",
    "#             if set_xlim: ax_main.set_xlim(set_xlim)\n",
    "#             if set_ylim and not use_log: ax_main.set_ylim(set_ylim)\n",
    "#             if not plot_data: ax_main.set_xlabel(xlabel, fontsize=20)\n",
    "\n",
    "#             # Legend\n",
    "#             handles, labels = ax_main.get_legend_handles_labels()\n",
    "#             by_label = dict(zip(labels, handles))\n",
    "#             ax_main.legend(by_label.values(), by_label.keys(), loc='upper right', ncol=1, frameon=False, fontsize=16)\n",
    "\n",
    "#         # SAVE\n",
    "#         fname = f\"{output_dir}/CMS_{region_name}_{variable}.png\"\n",
    "#         plt.savefig(fname, bbox_inches='tight', dpi=150)\n",
    "#         print(f\"Saved: {fname}\")\n",
    "#         plt.show()\n",
    "#         plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324fd335-4ceb-43da-99a4-944dcda13e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"GENERATING STACKED PLOTS ...\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Ensure output directory exists\n",
    "# output_dir = PLOTS_DIR / \"Stacked\"\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Loop over all variables defined in your configuration\n",
    "# for variable in VAR_LABELS.keys():\n",
    "#     try:\n",
    "#         stacked_plots(\n",
    "#             variable=variable, \n",
    "#             hist_data_all=hist_data_final, \n",
    "#             output_dir=output_dir\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"FAILED to plot {variable}: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# print(f\"\\nAll plots saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b57c4-2d7e-48e7-9abe-9deb2e5fa8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
