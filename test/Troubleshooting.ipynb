{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41f5630-3008-4d86-b386-fd6be23e8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5832df0-00fa-44de-bd65-6f4c08f1cb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR:    Data = 11174,  MC = 13810.4,  Data/MC = 0.809\n",
      "TopCR: Data = 1986,  MC = 2047.9,  Data/MC = 0.970\n",
      "\n",
      "Fakes fraction in SR: 36.0%\n"
     ]
    }
   ],
   "source": [
    "f = uproot.open(\"../Outputs/combine_input.root\")\n",
    "\n",
    "# SR\n",
    "data_sr = f[\"data_obs_SR\"].to_hist().sum().value\n",
    "mc_sr = sum(f[f\"{p}_SR\"].to_hist().sum().value \n",
    "            for p in [\"WW\",\"Top_antitop\",\"DY_to_Tau_Tau\",\"Fakes\",\"ggWW\",\"Diboson\",\"VG\",\"ggH_HWW\"])\n",
    "print(f\"SR:    Data = {data_sr:.0f},  MC = {mc_sr:.1f},  Data/MC = {data_sr/mc_sr:.3f}\")\n",
    "\n",
    "# TopCR\n",
    "data_cr = f[\"data_obs_TopCR\"].to_hist().sum().value\n",
    "mc_cr = sum(f[f\"{p}_TopCR\"].to_hist().sum().value \n",
    "            for p in [\"WW\",\"Top_antitop\",\"DY_to_Tau_Tau\",\"Fakes\",\"ggWW\",\"Diboson\",\"VG\",\"ggH_HWW\"])\n",
    "print(f\"TopCR: Data = {data_cr:.0f},  MC = {mc_cr:.1f},  Data/MC = {data_cr/mc_cr:.3f}\")\n",
    "\n",
    "# Check Fakes fraction\n",
    "fakes_sr = f[\"Fakes_SR\"].to_hist().sum().value\n",
    "print(f\"\\nFakes fraction in SR: {fakes_sr/mc_sr*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d90aea-e1f4-416e-90ee-6676293fd3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin  | Data\t | Fakes   | Fakes/Data\n",
      "   0 |      0.0 |      0.0 | -1.000\n",
      "   1 |    515.0 |    456.0 | 0.885\n",
      "   2 |    827.0 |    698.1 | 0.844\n",
      "   3 |   1020.0 |    745.5 | 0.731\n",
      "   4 |   1145.0 |    606.0 | 0.529\n",
      "   5 |   1150.0 |    473.4 | 0.412\n",
      "   6 |   1065.0 |    444.5 | 0.417\n",
      "   7 |    904.0 |    259.5 | 0.287\n",
      "   8 |    779.0 |    272.3 | 0.350\n",
      "   9 |    674.0 |    262.7 | 0.390\n",
      "  10 |    564.0 |    146.6 | 0.260\n",
      "  11 |    493.0 |    129.6 | 0.263\n",
      "  12 |    412.0 |    123.9 | 0.301\n",
      "  13 |    374.0 |     45.3 | 0.121\n",
      "  14 |    320.0 |     59.5 | 0.186\n",
      "  15 |    247.0 |     22.9 | 0.093\n",
      "  16 |    224.0 |     57.3 | 0.256\n",
      "  17 |    191.0 |     33.2 | 0.174\n",
      "  18 |    159.0 |    105.6 | 0.664\n",
      "  19 |    111.0 |     35.1 | 0.317\n"
     ]
    }
   ],
   "source": [
    "h_data = f[\"data_obs_SR\"].to_hist()\n",
    "h_fakes = f[\"Fakes_SR\"].to_hist()\n",
    "\n",
    "data_vals = h_data.view(flow=False).value\n",
    "fakes_vals = h_fakes.view(flow=False).value\n",
    "\n",
    "print(\"Bin  | Data\\t | Fakes   | Fakes/Data\")\n",
    "for i, (d, fk) in enumerate(zip(data_vals, fakes_vals)):\n",
    "    ratio = fk/d if d > 0 else -1\n",
    "    print(f\"  {i:2d} | {d:8.1f} | {fk:8.1f} | {ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "188f4211-53d9-484a-8541-2ac86e62bba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys:\n"
     ]
    }
   ],
   "source": [
    "g = uproot.open('fitDiagnostics.HWW.root')\n",
    "\n",
    "# Print all available keys to find the fit result\n",
    "print('Available keys:')\n",
    "for k in g.keys():\n",
    "    print(f'  {k}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bc9ecb-130c-46a7-ba61-e38b36fca34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C system headers (glibc/Xcode/Windows SDK) must be installed.\n",
      "In file included from input_line_4:36:\n",
      "/usr/local/bin/../lib/gcc/x86_64-conda-linux-gnu/13.4.0/include/c++/cassert:44:10: fatal error: 'assert.h' file not found\n",
      "#include <assert.h>\n",
      "         ^~~~~~~~~~\n",
      "input_line_37:1:10: fatal error: 'dlfcn.h' file not found\n",
      "#include \"dlfcn.h\"\n",
      "         ^~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIT RESULTS\n",
      "============================================================\n",
      "\n",
      "Signal strength:  r = 2.3414  -0.2170/+0.2210\n",
      "Fakes norm:       0.0000 +/- 0.0037\n",
      "VG norm:          0.0000 +/- 0.0080\n",
      "\n",
      "Parameter                           Value      Error\n",
      "----------------------------------------------------\n",
      "lumi_13TeV                         4.2021     0.8589\n",
      "CMS_fake_norm                     -0.0001     1.0000\n",
      "CMS_trigger                        0.0000     1.0000\n",
      "CMS_eff_e                          5.8416     0.7461\n",
      "CMS_eff_m                          1.8515     0.9793\n",
      "prop_binSR_bin1                    0.3651     0.9357\n",
      "prop_binSR_bin2                   -0.4705     0.9547\n",
      "prop_binSR_bin3                   -0.3493     0.9534\n",
      "prop_binSR_bin4                    0.0658     0.9385\n",
      "prop_binSR_bin5                    0.6493     0.9049\n",
      "prop_binSR_bin6                    1.0577     0.8959\n",
      "prop_binSR_bin7                    0.8235     0.9027\n",
      "prop_binSR_bin8                    0.6843     0.9142\n",
      "prop_binSR_bin9                    0.6702     0.9227\n",
      "prop_binSR_bin10                   0.1029     0.9227\n",
      "prop_binSR_bin11                   0.4804     0.9286\n",
      "prop_binSR_bin12                  -0.0291     0.9279\n",
      "prop_binSR_bin13                   0.6086     0.9207\n",
      "prop_binSR_bin14                   0.5517     0.9311\n",
      "prop_binSR_bin15                  -0.1188     0.9318\n",
      "prop_binSR_bin16                  -0.0612     0.9228\n",
      "prop_binSR_bin17                   0.1121     0.9373\n",
      "prop_binSR_bin18                  -0.1032     0.9316\n",
      "prop_binSR_bin19                  -0.7479     0.9510\n",
      "prop_binTopCR_bin5                 1.7041     0.8623\n",
      "prop_binTopCR_bin6                -0.1202     0.8758\n",
      "prop_binTopCR_bin7                 1.1661     0.8840\n",
      "prop_binTopCR_bin8                 0.8781     0.9008\n",
      "prop_binTopCR_bin9                 0.0027     0.9121\n",
      "prop_binTopCR_bin10                0.3709     0.9092\n",
      "prop_binTopCR_bin11                0.0535     0.9266\n",
      "prop_binTopCR_bin12               -0.1643     0.9139\n",
      "prop_binTopCR_bin13               -0.0918     0.8939\n",
      "prop_binTopCR_bin14                0.4283     0.9213\n",
      "prop_binTopCR_bin15               -0.0574     0.9282\n",
      "prop_binTopCR_bin16               -0.7299     0.9263\n",
      "prop_binTopCR_bin17               -0.5124     0.9314\n",
      "prop_binTopCR_bin18               -0.4198     0.9220\n",
      "prop_binTopCR_bin19                0.0058     0.9059\n",
      "CMS_norm_VG                        0.0000     0.0080\n",
      "CMS_norm_fake                      0.0000     0.0037\n",
      "r                                  2.3414     0.2190\n"
     ]
    }
   ],
   "source": [
    "# python3 << 'EOF'\n",
    "import ROOT\n",
    "\n",
    "f = ROOT.TFile.Open(\"fitDiagnostics.HWW.root\")\n",
    "fit_s = f.Get(\"fit_s\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FIT RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Signal strength\n",
    "r = fit_s.floatParsFinal().find(\"r\")\n",
    "print(f\"\\nSignal strength:  r = {r.getVal():.4f}  {r.getErrorLo():+.4f}/{r.getErrorHi():+.4f}\")\n",
    "\n",
    "# Fakes normalization\n",
    "fake_norm = fit_s.floatParsFinal().find(\"CMS_norm_fake\")\n",
    "if fake_norm:\n",
    "    print(f\"Fakes norm:       {fake_norm.getVal():.4f} +/- {fake_norm.getError():.4f}\")\n",
    "\n",
    "# VG normalization\n",
    "vg_norm = fit_s.floatParsFinal().find(\"CMS_norm_VG\")\n",
    "if vg_norm:\n",
    "    print(f\"VG norm:          {vg_norm.getVal():.4f} +/- {vg_norm.getError():.4f}\")\n",
    "\n",
    "# Print all floating parameters\n",
    "print(f\"\\n{'Parameter':<30} {'Value':>10} {'Error':>10}\")\n",
    "print(\"-\" * 52)\n",
    "pars = fit_s.floatParsFinal()\n",
    "for i in range(pars.getSize()):\n",
    "    p = pars[i]\n",
    "    print(f\"{p.GetName():<30} {p.getVal():>10.4f} {p.getError():>10.4f}\")\n",
    "\n",
    "f.Close()\n",
    "# EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f695a5b5-ff09-42dd-bf7e-3eb927e9ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FAKES SCALE FACTOR MEASUREMENT\n",
      "============================================================\n",
      "Region: SR_0jet\n",
      "\n",
      "  Data:                 11174.0\n",
      "  Total MC:             14587.0\n",
      "  MC (non-Fakes):        9535.2\n",
      "  MC Fakes:              5051.9\n",
      "\n",
      "  Data/MC:               0.7660\n",
      "  Excess over Data:      3413.0\n",
      "\n",
      "  >>> Fakes SF = 0.3244\n",
      "\n",
      "  Fakes MC is overestimated by ~3.1x\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "\n",
    "f = uproot.open(\"../Outputs/HWW_analysis_output.root\")\n",
    "\n",
    "VAR = \"mass\"\n",
    "REGION = \"SR_0jet\"\n",
    "\n",
    "# Load Data\n",
    "data_key = f\"Data_{REGION}_{VAR}_nominal\"\n",
    "h_data = f[data_key].to_hist()\n",
    "n_data = h_data.sum().value\n",
    "\n",
    "# Load all MC backgrounds\n",
    "mc_processes = {\n",
    "    \"ggH_HWW\": 0,\n",
    "    \"WW\": 0,\n",
    "    \"Top_antitop\": 0,\n",
    "    \"DY_to_Tau_Tau\": 0,\n",
    "    \"Fakes\": 0,\n",
    "    \"ggWW\": 0,\n",
    "    \"Diboson\": 0,\n",
    "    \"VG\": 0,\n",
    "}\n",
    "\n",
    "for proc in mc_processes:\n",
    "    key = f\"{proc}_{REGION}_{VAR}_nominal\"\n",
    "    if key in f:\n",
    "        mc_processes[proc] = f[key].to_hist().sum().value\n",
    "\n",
    "n_mc_total = sum(mc_processes.values())\n",
    "n_fakes = mc_processes[\"Fakes\"]\n",
    "n_mc_nonfakes = n_mc_total - n_fakes\n",
    "\n",
    "# The scale factor: how much to scale Fakes so that total MC = Data\n",
    "# Data = non-Fakes MC + SF * Fakes\n",
    "# SF = (Data - non-Fakes MC) / Fakes\n",
    "fakes_sf = (n_data - n_mc_nonfakes) / n_fakes\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FAKES SCALE FACTOR MEASUREMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"\")\n",
    "print(f\"  Data:              {n_data:>10.1f}\")\n",
    "print(f\"  Total MC:          {n_mc_total:>10.1f}\")\n",
    "print(f\"  MC (non-Fakes):    {n_mc_nonfakes:>10.1f}\")\n",
    "print(f\"  MC Fakes:          {n_fakes:>10.1f}\")\n",
    "print(f\"\")\n",
    "print(f\"  Data/MC:           {n_data/n_mc_total:>10.4f}\")\n",
    "print(f\"  Excess over Data:  {n_mc_total - n_data:>10.1f}\")\n",
    "print(f\"\")\n",
    "print(f\"  >>> Fakes SF = {fakes_sf:.4f}\")\n",
    "print(f\"\")\n",
    "if fakes_sf < 0:\n",
    "    print(\"  WARNING: SF is negative! Even without Fakes, MC > Data.\")\n",
    "    print(\"  This means other backgrounds are also overestimated.\")\n",
    "elif fakes_sf < 0.5:\n",
    "    print(f\"  Fakes MC is overestimated by ~{1/fakes_sf:.1f}x\")\n",
    "else:\n",
    "    print(f\"  Fakes need to be scaled to {fakes_sf*100:.1f}% of current value\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af8ea67-547d-4e4b-9dd9-b404b91585d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Outputs/HWW_analysis_output.root...\n",
      "\n",
      "--- Prompt MC Yields in Same-Sign Region ---\n",
      "  ggH_HWW        :     2.30\n",
      "  WW             :    71.90\n",
      "  Top_antitop    :    54.80\n",
      "  DY_to_Tau_Tau  :   368.23\n",
      "  ggWW           :     4.42\n",
      "  Diboson        :   227.91\n",
      "  VG             :  1014.99\n",
      "\n",
      "==================================================\n",
      " FAKES SCALE FACTOR CALCULATION\n",
      "==================================================\n",
      " Total Data Observed:          3217.00\n",
      " Total Prompt MC Expected:     1744.54\n",
      "--------------------------------------------------\n",
      " Target Fakes (Data - MC):     1472.46\n",
      " Fakes MC Prediction:          5008.15\n",
      "==================================================\n",
      " >>> CALCULATED FAKES SF:   0.2940 <<<\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_fakes_sf(root_file_path=\"Outputs/HWW_analysis_output.root\"):\n",
    "    print(f\"Opening {root_file_path}...\\n\")\n",
    "    \n",
    "    try:\n",
    "        f = uproot.open(root_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Could not find the ROOT file. Check the path.\")\n",
    "        return\n",
    "\n",
    "    # We use the leading_pt histogram, but any variable works because \n",
    "    # we are just summing the total area under the curve (the total yield)\n",
    "    region = \"CR_SS_0jet\"\n",
    "    var = \"leading_pt\" \n",
    "    \n",
    "    # 1. Get Data Yield\n",
    "    data_key = f\"Data_{region}_{var}_nominal\"\n",
    "    if data_key not in f:\n",
    "        print(f\"Error: Could not find {data_key} in the root file.\")\n",
    "        print(\"Make sure you successfully saved the Same-Sign region.\")\n",
    "        return\n",
    "        \n",
    "    n_data = f[data_key].to_hist().sum().value\n",
    "\n",
    "    # 2. Get Prompt MC Yields\n",
    "    prompt_samples = [\"ggH_HWW\", \"WW\", \"Top_antitop\", \"DY_to_Tau_Tau\", \"ggWW\", \"Diboson\", \"VG\"]\n",
    "    n_prompt = 0\n",
    "    \n",
    "    print(\"--- Prompt MC Yields in Same-Sign Region ---\")\n",
    "    for s in prompt_samples:\n",
    "        key = f\"{s}_{region}_{var}_nominal\"\n",
    "        if key in f:\n",
    "            yield_val = f[key].to_hist().sum().value\n",
    "            n_prompt += yield_val\n",
    "            print(f\"  {s:<15}: {yield_val:>8.2f}\")\n",
    "            \n",
    "    # 3. Get Fakes MC Yield\n",
    "    fakes_key = f\"Fakes_{region}_{var}_nominal\"\n",
    "    n_fakes_mc = f[fakes_key].to_hist().sum().value\n",
    "\n",
    "    # 4. Calculate the Scale Factor\n",
    "    # SF = (Observed Data - Prompt Backgrounds) / Simulated Fakes\n",
    "    data_driven_fakes = n_data - n_prompt\n",
    "    fakes_sf = data_driven_fakes / n_fakes_mc\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" FAKES SCALE FACTOR CALCULATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\" Total Data Observed:       {n_data:>10.2f}\")\n",
    "    print(f\" Total Prompt MC Expected:  {n_prompt:>10.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\" Target Fakes (Data - MC):  {data_driven_fakes:>10.2f}\")\n",
    "    print(f\" Fakes MC Prediction:       {n_fakes_mc:>10.2f}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\" >>> CALCULATED FAKES SF:   {fakes_sf:.4f} <<<\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    calculate_fakes_sf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5a6d9c-b9a1-4ed0-81aa-70a1a5cf4a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cms-jovyan/H-to-WW-NanoAOD-analysis/test\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4923db-1fd0-4adb-aa22-88207a855629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Yield         : 3,944.00\n",
      "WW MC Yield        : 2,318.34\n",
      "Other Backgrounds  : 2,148.68\n",
      "--------------------------------------\n",
      "WW Scale Factor    : 0.7744\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your newly generated cutflow\n",
    "df = pd.read_csv(\"Outputs/Cutflow_scaled.csv\", index_col=0)\n",
    "\n",
    "# 2. Isolate the 0-jet WW Control Region column\n",
    "cr_ww = df[\"CR WW 0j\"]\n",
    "\n",
    "# 3. Extract the exact yields\n",
    "data_yield = cr_ww[\"Data\"]\n",
    "\n",
    "# Combine standard WW and gluon-gluon WW (these are the processes we are scaling)\n",
    "ww_yield = cr_ww[\"WW\"] + cr_ww[\"ggWW\"]\n",
    "\n",
    "# Calculate all other backgrounds (Total MC minus the WW)\n",
    "total_mc = cr_ww[\"TOTAL (MC)\"]\n",
    "other_bkg = total_mc - ww_yield\n",
    "\n",
    "# 4. Calculate the Scale Factor\n",
    "ww_sf = (data_yield - other_bkg) / ww_yield\n",
    "\n",
    "print(f\"Data Yield         : {data_yield:,.2f}\")\n",
    "print(f\"WW MC Yield        : {ww_yield:,.2f}\")\n",
    "print(f\"Other Backgrounds  : {other_bkg:,.2f}\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f\"WW Scale Factor    : {ww_sf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fc5db-2482-480e-a0f7-a3a8f32a02de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
